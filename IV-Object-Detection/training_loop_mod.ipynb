{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "003c6a61-9b91-48d9-a90c-34de99de7a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b7d6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "# pip install torchsummary\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as fn\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "from matplotlib.patches import Polygon, Rectangle\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "# Own imports \n",
    "from config import * \n",
    "from utils import *\n",
    "from data_loader import TacoDataset\n",
    "from eval import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0c4e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 512\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(img_size, img_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=30, p=0.7),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.RGBShift(10, 10, 10, p=0.3),\n",
    "    A.GaussNoise(p=0.5),\n",
    "    A.Normalize(), # If you want to visualize - comment this line \n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',\n",
    "                            label_fields=['labels'],\n",
    "                            min_visibility=0.3, # min visibility of the original area in case of a crop\n",
    "                           )\n",
    ")\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(img_size, img_size),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',\n",
    "                            label_fields=['labels'],\n",
    "                           )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "de156f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "trainset = TacoDataset( 'train', transforms=train_transform, test_size=0.2) # test_transform for no augment\n",
    "valset   = TacoDataset('val', transforms=test_transform, test_size=0.2)\n",
    "testset  = TacoDataset('test', transforms=test_transform, test_size=0.2)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)\n",
    "val_loader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)\n",
    "test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f1bcbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_search(img):\n",
    "    \"\"\"\n",
    "    Takes image as an input (np.array not Tensor!)\n",
    "    Returns np.array (number of bboxes x 4)\n",
    "    Bboxes in format x, y, w, h (see demo notebook for example)\n",
    "    \"\"\"\n",
    "    # create selective search segmentation object\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    ss.setBaseImage(img) \n",
    "    # Choose between fast or accurate selective Search method: fast but low recall V.S. high recall but slow \n",
    "    ss.switchToSelectiveSearchFast()\n",
    "    # AM: Quality takes a looong time, maybe better to try with fast for now and see the results, if bad then change to quality\n",
    "    # ss.switchToSelectiveSearchQuality() \n",
    "    # run selective search\n",
    "    rects = ss.process()\n",
    "\n",
    "    # get rectangles to x1, y1, x2, y2 as this is format in IoU \n",
    "    rects[:, 2] = rects[:, 0] + rects[:, 2]\n",
    "    rects[:, 3] = rects[:, 1] + rects[:, 3]\n",
    "\n",
    "    rects = rects[:250, :]\n",
    "    \n",
    "    print('Total Number of Region Proposals: {}'.format(len(rects))) # TODO: comment out after making the whole trainset work\n",
    "    return rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20a8d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_model_set(model, freeze_convs=False,):\n",
    "    \n",
    "    if freeze_convs:\n",
    "        print('Freezing Convs')\n",
    "        # freeze the feature extractors\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    if type(model) == models.densenet.DenseNet:\n",
    "        in_features = model.classifier.in_features\n",
    "    \n",
    "    elif type(model) == models.resnet.ResNet:\n",
    "        in_features = model.fc.in_features\n",
    "    \n",
    "    \n",
    "    size_hidden = 512\n",
    "    out_features = 1\n",
    "    \n",
    "    head = nn.Sequential(\n",
    "                    nn.Linear(in_features, size_hidden),\n",
    "                    nn.Dropout(DROP_OUT_RATE),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm1d(size_hidden),\n",
    "                    nn.Linear(size_hidden, out_features),\n",
    "                    nn.Sigmoid()        \n",
    "    )\n",
    "                    \n",
    "    \n",
    "    if type(model) == models.densenet.DenseNet:\n",
    "        model.classifier = head\n",
    "    \n",
    "    elif type(model) == models.resnet.ResNet:\n",
    "        model.fc = head\n",
    "\n",
    "    else:\n",
    "        raise Exception('Not implemented the classifier for this type of model')\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8513dc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing Convs\n"
     ]
    }
   ],
   "source": [
    "HEAD_LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 5\n",
    "loss = nn.BCELoss()\n",
    "DROP_OUT_RATE = 0.5\n",
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "model = transfer_model_set(model, freeze_convs=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), HEAD_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c2a8114d-f1d0-4b8d-b3d4-1cb319e30afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e4de27df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0130d2a08b9e4575be379cd543b52296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbadbf7c13854a76a89e2b1648f8c2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Region Proposals: 250\n",
      "IoU processing\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.09074793902215067]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.006745202167354463]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.023071726674227955]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.030931729610462307]\n",
      "[0.13846094160060082]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0322806376938127]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.14106183554199223]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.07468824305279792]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.44122273800424977]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.030902861054514206]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.15217984692643777]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0597849398639577]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.04979659119198144]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.07967487664244668]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "[0.0]\n",
      "Total Number of Region Proposals: 250\n",
      "IoU processing\n",
      "[0.003993499839771221]\n",
      "[0.0]\n",
      "[0.15095429394335214]\n",
      "[0.0]\n",
      "[0.0034107748631515425]\n",
      "[0.0017114999313305231]\n",
      "[0.02339864906119015]\n",
      "[0.003871249844676183]\n",
      "[0.016444509606867132]\n",
      "[0.3849141837223351]\n",
      "[0.04105691497119712]\n",
      "[0.004433599822113355]\n",
      "[0.038406873459024236]\n",
      "[0.014975624399142077]\n",
      "[0.006181774751972389]\n",
      "[0.1096582456002485]\n",
      "[0.0]\n",
      "[0.004400999823421345]\n",
      "[0.11257408990573259]\n",
      "[0.0013936499440834259]\n",
      "[0.0012224999509503735]\n",
      "[0.04979649800204522]\n",
      "[0.29428523092981895]\n",
      "[0.0011695249530758573]\n",
      "[0.0031999932857759133]\n",
      "[0.23490289042704293]\n",
      "[0.07515784600351733]\n",
      "[0.06483324739873482]\n",
      "[0.0022819999084406975]\n",
      "[0.0010431999581443187]\n",
      "[0.003655274853341617]\n",
      "[0.0018337499264255604]\n",
      "[0.005052999797261544]\n",
      "[0.009600699614796935]\n",
      "[0.003911999843041195]\n",
      "[0.0009800694799560338]\n",
      "[0.008895724643082218]\n",
      "[0.0036674998528511207]\n",
      "[0.0017807749285510442]\n",
      "[0.003671574852687622]\n",
      "[0.0030317998783569266]\n",
      "[0.006658549732843035]\n",
      "[0.012771324599993524]\n",
      "[0.007436874701614773]\n",
      "[0.0017114999313305231]\n",
      "[0.008468649959485527]\n",
      "[0.002444999901900747]\n",
      "[0.261761689497494]\n",
      "[0.02349644905726618]\n",
      "[0.010187499591253113]\n",
      "[0.00860639965469063]\n",
      "[0.004563999816881395]\n",
      "[0.023500524057102682]\n",
      "[0.012632482381848387]\n",
      "[0.0021515999136726576]\n",
      "[0.004380624824238838]\n",
      "[0.002310524907296206]\n",
      "[0.033716548647211304]\n",
      "[0.0015240499388514657]\n",
      "[0.2591699896014792]\n",
      "[0.011258100455884759]\n",
      "[0.0028684364215450075]\n",
      "[0.002396099903862732]\n",
      "[0.012196474510648227]\n",
      "[0.0027870121304369084]\n",
      "[0.0017522499296955354]\n",
      "[0.12410819502048193]\n",
      "[0.0017318749305130293]\n",
      "[0.006821549726303085]\n",
      "[0.00333192723334207]\n",
      "[0.0006608907809747858]\n",
      "[0.0015857148369544194]\n",
      "[0.006275499748211918]\n",
      "[0.002689499892090822]\n",
      "[0.003911999843041195]\n",
      "[0.0026283748945433033]\n",
      "[0.011344799544819466]\n",
      "[0.001695199931984518]\n",
      "[0.0021515999136726576]\n",
      "[0.045971772789836975]\n",
      "[0.0011613749534028549]\n",
      "[0.0025998498956877943]\n",
      "[0.003390399863969036]\n",
      "[0.0029419747065353888]\n",
      "[0.09843569605052409]\n",
      "[0.0022173478467337497]\n",
      "[0.002020802302514698]\n",
      "[0.02574992396685137]\n",
      "[0.07094027601288298]\n",
      "[0.0017726249288780417]\n",
      "[0.028862823211900205]\n",
      "[0.000493799183951584]\n",
      "[0.006723749730227054]\n",
      "[0.0016299999346004982]\n",
      "[0.0015077499395054607]\n",
      "[0.0012224999509503735]\n",
      "[0.001593324936071987]\n",
      "[0.0035045640249723392]\n",
      "[0.00444989982145936]\n",
      "[0.14113753440210797]\n",
      "[0.002506124899448266]\n",
      "[0.005704999771101744]\n",
      "[0.012133949792676268]\n",
      "[0.0023693458235861343]\n",
      "[0.00117767495274886]\n",
      "[0.0022453249099121863]\n",
      "[0.0051711747925200805]\n",
      "[0.012408374502146291]\n",
      "[0.0034107748631515425]\n",
      "[0.036734762999026185]\n",
      "[0.006189924751645391]\n",
      "[0.0014832999404864532]\n",
      "[0.0022167999110566773]\n",
      "[0.006161399752789883]\n",
      "[0.0014995999398324583]\n",
      "[0.0010431999581443187]\n",
      "[0.016943849320172177]\n",
      "[0.008557499656652616]\n",
      "[0.022591799093562903]\n",
      "[0.0011817499525853612]\n",
      "[0.0032273998705089862]\n",
      "[0.003194799871816976]\n",
      "[0.05887559763776999]\n",
      "[0.0909825213495633]\n",
      "[0.07525709698050499]\n",
      "[0.08590370272820717]\n",
      "[0.0011409999542203487]\n",
      "[0.88390106224256]\n",
      "[0.002444999901900747]\n",
      "[0.006931574721888619]\n",
      "[0.05613719774764116]\n",
      "[0.029189828608456123]\n",
      "[0.0025957748958512932]\n",
      "[0.0037489998495811457]\n",
      "[0.001597399935908488]\n",
      "[0.004792199807725464]\n",
      "[0.005341996037254288]\n",
      "[0.0022453249099121863]\n",
      "[0.0]\n",
      "[0.003025818414746267]\n",
      "[0.0906442963631337]\n",
      "[0.11888356870398444]\n",
      "[0.0019070999234825827]\n",
      "[0.00381895562384829]\n",
      "[0.0023471999058247172]\n",
      "[0.04849998303261944]\n",
      "[0.002841992838113436]\n",
      "[0.014576274415164955]\n",
      "[0.02548504897747879]\n",
      "[0.00595764976096482]\n",
      "[0.0017929999280605479]\n",
      "[0.004185909663195661]\n",
      "[0.001528124938687967]\n",
      "[0.015959839413736025]\n",
      "[0.0031662748729614676]\n",
      "[0.03557474857265587]\n",
      "[0.0022860749082771986]\n",
      "[0.004841099805763479]\n",
      "[0.0015484999378704732]\n",
      "[0.0025427998979767772]\n",
      "[0.00117767495274886]\n",
      "[0.004400999823421345]\n",
      "[0.004767749808706457]\n",
      "[0.0014485071833279359]\n",
      "[0.004171267274682209]\n",
      "[0.02172738701817985]\n",
      "[0.0008101166202274662]\n",
      "[0.001511824939341962]\n",
      "[0.028924348839485838]\n",
      "[0.0075631996965463116]\n",
      "[0.0026487498937258097]\n",
      "[0.006075824756223357]\n",
      "[0.12776754487366004]\n",
      "[0.0006234878711432063]\n",
      "[0.23728012557610134]\n",
      "[0.3129562768628208]\n",
      "[0.00847599965992259]\n",
      "[0.0009355404969081238]\n",
      "[0.0030480998777029315]\n",
      "[0.0186471992518297]\n",
      "[0.023678092244570015]\n",
      "[0.003948674841569706]\n",
      "[0.0021026999156346427]\n",
      "[0.0018581999254445678]\n",
      "[0.0015647999372164782]\n",
      "[0.024462224018516974]\n",
      "[0.022860749082771987]\n",
      "[0.002444999901900747]\n",
      "[0.0020048999195586125]\n",
      "[0.08423024662048074]\n",
      "[0.0012713999489883886]\n",
      "[0.004074999836501245]\n",
      "[0.003535512119387384]\n",
      "[0.029083273833109387]\n",
      "[0.007900139932429768]\n",
      "[0.0006321492366803071]\n",
      "[0.022037599115798733]\n",
      "[0.01158114953533654]\n",
      "[0.032938223678439564]\n",
      "[0.004400999823421345]\n",
      "[0.0036756498525241234]\n",
      "[0.006040462355934295]\n",
      "[0.005195624791539088]\n",
      "[0.0022004999117106725]\n",
      "[0.008035899677580456]\n",
      "[0.0023390499061517146]\n",
      "[0.041075998351932555]\n",
      "[0.0015484999378704732]\n",
      "[0.01442680382574593]\n",
      "[0.0040342498381362325]\n",
      "[0.00448249982015137]\n",
      "[0.013186378865599391]\n",
      "[0.0801586427788771]\n",
      "[0.0011735999529123586]\n",
      "[0.001026899958798314]\n",
      "[0.0031662748729614676]\n",
      "[0.0014547749416309445]\n",
      "[0.0016002637104920716]\n",
      "[0.005281199788105614]\n",
      "[0.0008557499656652616]\n",
      "[0.27597936392704686]\n",
      "[0.0026487498937258097]\n",
      "[0.06269290560195227]\n",
      "[0.004433599822113355]\n",
      "[0.004563999816881395]\n",
      "[0.0012713999489883886]\n",
      "[0.0031662748729614676]\n",
      "[0.0012713999489883886]\n",
      "[0.006544449737421]\n",
      "[0.0036258787842541416]\n",
      "[0.005810949766850776]\n",
      "[0.028347758600208984]\n",
      "[0.002555024897486281]\n",
      "[0.002053799917596628]\n",
      "[0.0038427248458206744]\n",
      "[0.0010657147617111546]\n",
      "[0.023993599037319333]\n",
      "[0.001198049951931366]\n",
      "[0.003190724871980475]\n",
      "[0.004413224822930849]\n",
      "[0.06229010314368817]\n",
      "[0.001426249942775436]\n",
      "[0.013117424473697509]\n",
      "[0.002823974886695363]\n",
      "[0.001528124938687967]\n",
      "[0.002823974886695363]\n",
      "[0.001426249942775436]\n",
      "[0.022421396656747604]\n",
      "[0.0696879061611196]\n",
      "[0.000629841333037902]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fbb54dcf760>\n",
      "Traceback (most recent call last):\n",
      "  File \"/zhome/8d/e/198218/dlincv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/zhome/8d/e/198218/dlincv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1442, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/appl/python/3.10.11/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/appl/python/3.10.11/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/appl/python/3.10.11/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/appl/python/3.10.11/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m cropped_images_all \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, img_bboxes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(images, bboxes):\n\u001b[0;32m---> 32\u001b[0m     proposals \u001b[38;5;241m=\u001b[39m \u001b[43mselective_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# .cpu()\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     proposals_all\u001b[38;5;241m.\u001b[39mappend(proposals)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# IoU\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[65], line 15\u001b[0m, in \u001b[0;36mselective_search\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     11\u001b[0m ss\u001b[38;5;241m.\u001b[39mswitchToSelectiveSearchFast()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# AM: Quality takes a looong time, maybe better to try with fast for now and see the results, if bad then change to quality\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ss.switchToSelectiveSearchQuality() \u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# run selective search\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m rects \u001b[38;5;241m=\u001b[39m \u001b[43mss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# get rectangles to x1, y1, x2, y2 as this is format in IoU \u001b[39;00m\n\u001b[1;32m     18\u001b[0m rects[:, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m rects[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m rects[:, \u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 0.5 # Threshold for classification\n",
    "p = 0.05 # Probability of cropping background image\n",
    "num_epochs = NUM_EPOCHS\n",
    "\n",
    "# def train(model, train_loader, test_loader, loss_function, optimizer, num_epochs, model_name, lr_scheduler=None, save_model=False ):\n",
    "\n",
    "#     def loss_fun(output, target):\n",
    "#         return F.cross_entropy(output, target)\n",
    "\n",
    "out_dict = {'train_acc': [],\n",
    "            'test_acc': [],\n",
    "            'train_loss': [],\n",
    "            'test_loss': []}\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), unit='epoch'):\n",
    "    model.train()\n",
    "    train_correct = 0\n",
    "    train_len = 0\n",
    "    train_loss = []\n",
    "    for minibatch_no, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        images = [image for image, _, _ in batch]\n",
    "        bboxes = [bbox for _, bbox, _ in batch]\n",
    "        labels = [label for _, _, label in batch]\n",
    "        # images, bboxes, labels = torch.FloatTensor(images).to(device), torch.FloatTensor(bboxes).to(device), torch.FloatTensor(labels).to(device)\n",
    "        # print(images.shape)\n",
    "        \n",
    "        # Selective search\n",
    "        proposals_all = []\n",
    "        predictions_all = []\n",
    "        cropped_images_all = []\n",
    "        for image, img_bboxes in zip(images, bboxes):\n",
    "            proposals = selective_search(image.permute([1,2,0]).numpy()) # .cpu()\n",
    "            proposals_all.append(proposals)\n",
    "            \n",
    "            # IoU\n",
    "            print('IoU processing')\n",
    "            for proposal in proposals:\n",
    "\n",
    "                proposal_wh = copy.copy(proposal)\n",
    "                proposal_wh[2] = proposal_wh[2] - proposal_wh[0]\n",
    "                proposal_wh[3] = proposal_wh[3] - proposal_wh[1]\n",
    "                \n",
    "                scores_all = []\n",
    "                for bbox in img_bboxes:\n",
    "                    score = IoU(proposal, bbox)\n",
    "                    scores_all.append(score)\n",
    "                \n",
    "                prediction = max(scores_all) > k\n",
    "                print(scores_all)\n",
    "                # Extract image\n",
    "                if prediction or random.random() < p:\n",
    "                    cropped_image = fn.crop(image, *proposal_wh)\n",
    "                    resized_image = fn.resize(cropped_image, size=[300, 300]) # pretrained size\n",
    "                    cropped_images_all.append(resized_image)\n",
    "                    predictions_all.append(prediction)\n",
    "        # print(f\"Len: {len(predictions_all)}, sum: {sum(predictions_all)}\")\n",
    "        \n",
    "        data, target = torch.stack(cropped_images_all).to(device), torch.FloatTensor(predictions_all).to(device)\n",
    "        print('stacked images')\n",
    "        # CNN\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(data)[:,0]\n",
    "#             loss = loss_function(output, target)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             if lr_scheduler is not None:\n",
    "#                 lr_scheduler.step()\n",
    "#             train_loss.append(loss.item())\n",
    "#             predicted = output > 0.5\n",
    "#             train_correct += (target==predicted).sum().cpu().item()\n",
    "#             train_len += data.shape[0]\n",
    "        \n",
    "#         test_loss = []\n",
    "#         test_correct = 0\n",
    "#         test_len = 0\n",
    "#         model.eval()\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(torch.float32).to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 output = model(data)[:,0]\n",
    "#             test_loss.append(loss_function(output, target).cpu().item())\n",
    "#             predicted = output > 0.5\n",
    "#             test_correct += (target==predicted).sum().cpu().item()\n",
    "#             test_len += data.shape[0]\n",
    "\n",
    "#         if save_model and epoch > 0 and test_correct/test_len > max(out_dict['test_acc']):\n",
    "#             torch.save(model, 'models/' + model_name)\n",
    "        \n",
    "        \n",
    "#         out_dict['train_acc'].append(train_correct/train_len)\n",
    "#         out_dict['test_acc'].append(test_correct/test_len)\n",
    "#         out_dict['train_loss'].append(np.mean(train_loss))\n",
    "#         out_dict['test_loss'].append(np.mean(test_loss))\n",
    "\n",
    "    \n",
    "#         print(f\"Loss train: {np.mean(train_loss):.3f}\\t test: {np.mean(test_loss):.3f}\\t\",\n",
    "#               f\"Accuracy train: {out_dict['train_acc'][-1]*100:.1f}%\\t test: {out_dict['test_acc'][-1]*100:.1f}%\")\n",
    "\n",
    "# return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1a23505e-4424-4ea6-903a-e8b9aa98679e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 64, 498,  74, 512], dtype=int32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c39bf35e-86f1-4eda-ae2f-9f47c9aebcba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 64, 498,  10,  14], dtype=int32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proposal_wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "229a6905-7706-4bb7-911b-d11b4cd5babc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a185d03b-a364-4462-b546-9dd03e3d06ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(146.86839320457761,\n",
       "  15.894666336524068,\n",
       "  268.1166215125553,\n",
       "  77.78430964029158),\n",
       " (144.1386701465699, 224.5796505222911, 422.1630538010227, 448.076537534678)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eab177f8-5859-48b0-87b2-ff945e32355b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([398, 448, 102,  64], dtype=int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c969f4f-97d9-4dd7-9382-515e5742dea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 3, 300, 300])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da863d8d-8021-4064-b692-d7294390ebf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2eaef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_dict = train(model,\n",
    "#                           train_loader,\n",
    "#                           test_loader,\n",
    "#                           loss,\n",
    "#                           optimizer,\n",
    "#                           NUM_EPOCHS, \n",
    "#                           save_model=True, \n",
    "#                           model_name='densenet121_full_Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7038041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22d5d26b00941b69d804def31e98e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for it, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "    batch = data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7b2df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [image for image, _, _ in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8e27700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1024, 1024])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(images).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
