{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7d6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "# pip install torchsummary\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as fn\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "from matplotlib.patches import Polygon, Rectangle\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "# Own imports \n",
    "from config import * \n",
    "from utils import *\n",
    "from data_loader import TacoDataset\n",
    "from eval import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0c4e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 1024\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(img_size, img_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=30, p=0.7),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.RGBShift(10, 10, 10, p=0.3),\n",
    "    A.GaussNoise(p=0.5),\n",
    "    A.Normalize(), # If you want to visualize - comment this line \n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',\n",
    "                            label_fields=['labels'],\n",
    "                            min_visibility=0.3, # min visibility of the original area in case of a crop\n",
    "                           )\n",
    ")\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(img_size, img_size),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',\n",
    "                            label_fields=['labels'],\n",
    "                           )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de156f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "trainset = TacoDataset( 'train', transforms=train_transform, test_size=0.2) # test_transform for no augment\n",
    "valset   = TacoDataset('val', transforms=test_transform, test_size=0.2)\n",
    "testset  = TacoDataset('test', transforms=test_transform, test_size=0.2)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8,collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)\n",
    "val_loader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8,collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)\n",
    "test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8,collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1bcbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_search(img):\n",
    "    \"\"\"\n",
    "    Takes image as an input (np.array not Tensor!)\n",
    "    Returns np.array (number of bboxes x 4)\n",
    "    Bboxes in format x, y, w, h (see demo notebook for example)\n",
    "    \"\"\"\n",
    "    # create selective search segmentation object\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    ss.setBaseImage(img) \n",
    "    # Choose between fast or accurate selective Search method: fast but low recall V.S. high recall but slow \n",
    "    ss.switchToSelectiveSearchFast()\n",
    "    # AM: Quality takes a looong time, maybe better to try with fast for now and see the results, if bad then change to quality\n",
    "    # ss.switchToSelectiveSearchQuality() \n",
    "    # run selective search\n",
    "    rects = ss.process()\n",
    "    print('Total Number of Region Proposals: {}'.format(len(rects))) # TODO: comment out after making the whole trainset work\n",
    "    return rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20a8d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_model_set(model, freeze_convs=False,):\n",
    "    \n",
    "    if freeze_convs:\n",
    "        print('Freezing Convs')\n",
    "        # freeze the feature extractors\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    if type(model) == models.densenet.DenseNet:\n",
    "        in_features = model.classifier.in_features\n",
    "    \n",
    "    elif type(model) == models.resnet.ResNet:\n",
    "        in_features = model.fc.in_features\n",
    "    \n",
    "    \n",
    "    size_hidden = 512\n",
    "    out_features = 1\n",
    "    \n",
    "    head = nn.Sequential(\n",
    "                    nn.Linear(in_features, size_hidden),\n",
    "                    nn.Dropout(DROP_OUT_RATE),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm1d(size_hidden),\n",
    "                    nn.Linear(size_hidden, out_features),\n",
    "                    nn.Sigmoid()        \n",
    "    )\n",
    "                    \n",
    "    \n",
    "    if type(model) == models.densenet.DenseNet:\n",
    "        model.classifier = head\n",
    "    \n",
    "    elif type(model) == models.resnet.ResNet:\n",
    "        model.fc = head\n",
    "\n",
    "    else:\n",
    "        raise Exception('Not implemented the classifier for this type of model')\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8513dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD_LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 5\n",
    "loss = nn.BCELoss()\n",
    "DROP_OUT_RATE = 0.5\n",
    "\n",
    "model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "model = transfer_model_set(model, freeze_convs=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), HEAD_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4de27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0.5 # Threshold for classification\n",
    "p = 0.05 # Probability of cropping background image\n",
    "\n",
    "def train(model, train_loader, test_loader, loss_function, optimizer, num_epochs, model_name, lr_scheduler=None, save_model=False ):\n",
    "    \n",
    "#     def loss_fun(output, target):\n",
    "#         return F.cross_entropy(output, target)\n",
    "    \n",
    "    out_dict = {'train_acc': [],\n",
    "                'test_acc': [],\n",
    "                'train_loss': [],\n",
    "                'test_loss': []}\n",
    "  \n",
    "    for epoch in tqdm(range(num_epochs), unit='epoch'):\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_len = 0\n",
    "        train_loss = []\n",
    "        for minibatch_no, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            images = [image for image, _, _ in batch]\n",
    "            bboxes = [bbox for _, bbox, _ in batch]\n",
    "            labels = [label for _, _, label in batch]\n",
    "            # images, bboxes, labels = torch.FloatTensor(images).to(device), torch.FloatTensor(bboxes).to(device), torch.FloatTensor(labels).to(device)\n",
    "            # print(images.shape)\n",
    "            \n",
    "            # Selective search\n",
    "            proposals_all = []\n",
    "            predictions_all = []\n",
    "            cropped_images_all = []\n",
    "            for image, img_bboxes in zip(images, bboxes):\n",
    "                proposals = selective_search(image.permute([1,2,0]).numpy()) # .cpu()\n",
    "                proposals_all.append(proposals)\n",
    "                \n",
    "                # IoU\n",
    "                for proposal in proposals:\n",
    "                    scores_all = []\n",
    "                    for bbox in img_bboxes:\n",
    "                        score = IoU(proposal, bbox)\n",
    "                        scores_all.append(score)\n",
    "                    \n",
    "                    prediction = max(scores_all) > k\n",
    "                    \n",
    "                    # Extract image\n",
    "                    if prediction or random.random() < p:\n",
    "                        cropped_image = fn.crop(image, *proposal)\n",
    "                        resized_image = fn.resize(cropped_image, size=[1024, 1024])\n",
    "                        cropped_images_all.append(resized_image)\n",
    "                        predictions_all.append(prediction)\n",
    "                        \n",
    "            # print(f\"Len: {len(predictions_all)}, sum: {sum(predictions_all)}\")\n",
    "            \n",
    "            data, target = torch.stack(cropped_images_all).to(device), torch.FloatTensor(predictions_all).to(device)\n",
    "            \n",
    "            \n",
    "            # CNN\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(data)[:,0]\n",
    "#             loss = loss_function(output, target)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             if lr_scheduler is not None:\n",
    "#                 lr_scheduler.step()\n",
    "#             train_loss.append(loss.item())\n",
    "#             predicted = output > 0.5\n",
    "#             train_correct += (target==predicted).sum().cpu().item()\n",
    "#             train_len += data.shape[0]\n",
    "            \n",
    "#         test_loss = []\n",
    "#         test_correct = 0\n",
    "#         test_len = 0\n",
    "#         model.eval()\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(torch.float32).to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 output = model(data)[:,0]\n",
    "#             test_loss.append(loss_function(output, target).cpu().item())\n",
    "#             predicted = output > 0.5\n",
    "#             test_correct += (target==predicted).sum().cpu().item()\n",
    "#             test_len += data.shape[0]\n",
    "\n",
    "#         if save_model and epoch > 0 and test_correct/test_len > max(out_dict['test_acc']):\n",
    "#             torch.save(model, 'models/' + model_name)\n",
    "            \n",
    "            \n",
    "#         out_dict['train_acc'].append(train_correct/train_len)\n",
    "#         out_dict['test_acc'].append(test_correct/test_len)\n",
    "#         out_dict['train_loss'].append(np.mean(train_loss))\n",
    "#         out_dict['test_loss'].append(np.mean(test_loss))\n",
    "\n",
    "        \n",
    "#         print(f\"Loss train: {np.mean(train_loss):.3f}\\t test: {np.mean(test_loss):.3f}\\t\",\n",
    "#               f\"Accuracy train: {out_dict['train_acc'][-1]*100:.1f}%\\t test: {out_dict['test_acc'][-1]*100:.1f}%\")\n",
    "        \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f2eaef8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1229cdce61a24c949d35f8102e1a4dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396bb663b89c4493940c30a502857db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Region Proposals: 1702\n",
      "Total Number of Region Proposals: 8866\n",
      "torch.Size([539, 3, 1024, 1024])\n",
      "torch.Size([539])\n",
      "tensor(0., device='cuda:0')\n",
      "Total Number of Region Proposals: 8539\n",
      "Total Number of Region Proposals: 7981\n",
      "torch.Size([841, 3, 1024, 1024])\n",
      "torch.Size([841])\n",
      "tensor(13., device='cuda:0')\n",
      "Total Number of Region Proposals: 6441\n",
      "Total Number of Region Proposals: 6371\n",
      "torch.Size([605, 3, 1024, 1024])\n",
      "torch.Size([605])\n",
      "tensor(3., device='cuda:0')\n",
      "Total Number of Region Proposals: 9131\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                          \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdensenet121_full_Adam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, loss_function, optimizer, num_epochs, model_name, lr_scheduler, save_model)\u001b[0m\n\u001b[1;32m     29\u001b[0m cropped_images_all \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, img_bboxes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(images, bboxes):\n\u001b[0;32m---> 31\u001b[0m     proposals \u001b[38;5;241m=\u001b[39m \u001b[43mselective_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# .cpu()\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     proposals_all\u001b[38;5;241m.\u001b[39mappend(proposals)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# IoU\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mselective_search\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     11\u001b[0m ss\u001b[38;5;241m.\u001b[39mswitchToSelectiveSearchFast()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# AM: Quality takes a looong time, maybe better to try with fast for now and see the results, if bad then change to quality\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ss.switchToSelectiveSearchQuality() \u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# run selective search\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m rects \u001b[38;5;241m=\u001b[39m \u001b[43mss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Number of Region Proposals: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(rects))) \u001b[38;5;66;03m# TODO: comment out after making the whole trainset work\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rects\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out_dict = train(model,\n",
    "                          train_loader,\n",
    "                          test_loader,\n",
    "                          loss,\n",
    "                          optimizer,\n",
    "                          NUM_EPOCHS, \n",
    "                          save_model=True, \n",
    "                          model_name='densenet121_full_Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7038041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22d5d26b00941b69d804def31e98e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for it, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "    batch = data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7b2df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [image for image, _, _ in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8e27700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1024, 1024])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(images).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
