{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ffd46c-e3db-457c-b762-750d061a5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7d6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "# pip install torchsummary\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as fn\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "from matplotlib.patches import Polygon, Rectangle\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "# Own imports \n",
    "from config import * \n",
    "from utils import *\n",
    "from data_loader import TacoDataset\n",
    "from eval import *\n",
    "\n",
    "# speed-up using multithreads\n",
    "cv2.setUseOptimized(True);\n",
    "cv2.setNumThreads(8);\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c4e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_size = 512\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=30, p=0.7),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.RGBShift(10, 10, 10, p=0.3),\n",
    "    A.GaussNoise(p=0.5),\n",
    "    #A.Normalize(), # If you want to visualize - comment this line \n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='coco',\n",
    "                            label_fields=['labels'],\n",
    "                            min_visibility=0.3, # min visibility of the original area in case of a crop\n",
    "                           )\n",
    ")\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    #A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='coco',\n",
    "                            label_fields=['labels'],\n",
    "                           )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a040fe49-393f-44eb-bdfd-f0a5d9110564",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de156f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TacoDataset( 'train', transforms=train_transform, test_size=0.2) # test_transform for no augment\n",
    "valset   = TacoDataset('val', transforms=test_transform, test_size=0.2)\n",
    "testset  = TacoDataset('test', transforms=test_transform, test_size=0.2)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=12, collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)\n",
    "val_loader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False, num_workers=12, collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)\n",
    "test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=12, collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8513dc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing Convs\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "\n",
    "model = models.resnet34(weights=models.ResNet34_Weights)\n",
    "model = transfer_model_set(model, freeze_convs=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), HEAD_LEARNING_RATE)\n",
    "\n",
    "lr_scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef75c06c-0cfb-4319-92f4-271666a7a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72397995-c9c9-40bb-83a8-22dea7a6cef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e17b99374644160822f3f1dd8fd5ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/zhome/7c/2/136917/Desktop/DLiCV/Project1/2023-DTU-Deep-Learning-In-Computer-Vision/IV-Object-Detection/data_loader.py\", line 141, in __getitem__\n    x = self.transforms(image=image,\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/albumentations/core/composition.py\", line 207, in __call__\n    p.preprocess(data)\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/albumentations/core/utils.py\", line 83, in preprocess\n    data[data_name] = self.check_and_convert(data[data_name], rows, cols, direction=\"to\")\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/albumentations/core/utils.py\", line 91, in check_and_convert\n    return self.convert_to_albumentations(data, rows, cols)\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/albumentations/core/bbox_utils.py\", line 142, in convert_to_albumentations\n    return convert_bboxes_to_albumentations(data, self.params.format, rows, cols, check_validity=True)\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/albumentations/core/bbox_utils.py\", line 408, in convert_bboxes_to_albumentations\n    return [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) for bbox in bboxes]\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/albumentations/core/bbox_utils.py\", line 408, in <listcomp>\n    return [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) for bbox in bboxes]\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/albumentations/core/bbox_utils.py\", line 352, in convert_bbox_to_albumentations\n    check_bbox(bbox)\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/albumentations/core/bbox_utils.py\", line 435, in check_bbox\n    raise ValueError(f\"Expected {name} for bbox {bbox} to be in the range [0.0, 1.0], got {value}.\")\nValueError: Expected y_max for bbox (tensor(0.4004), tensor(0.6511), tensor(0.9601), tensor(1.3481), tensor(1)) to be in the range [0.0, 1.0], got 1.348055124282837.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m train_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m minibatch_no, batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)):\n\u001b[1;32m     11\u001b[0m     images \u001b[38;5;241m=\u001b[39m [image \u001b[38;5;28;01mfor\u001b[39;00m image, _, _ \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m     12\u001b[0m     bboxes \u001b[38;5;241m=\u001b[39m [bbox \u001b[38;5;28;01mfor\u001b[39;00m _, bbox, _ \u001b[38;5;129;01min\u001b[39;00m batch]\n",
      "File \u001b[0;32m~/Desktop/venv_1/lib/python3.10/site-packages/tqdm/notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/venv_1/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/venv_1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Desktop/venv_1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/venv_1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/venv_1/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/zhome/7c/2/136917/Desktop/DLiCV/Project1/2023-DTU-Deep-Learning-In-Computer-Vision/IV-Object-Detection/data_loader.py\", line 141, in __getitem__\n    x = self.transforms(image=image,\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/albumentations/core/composition.py\", line 207, in __call__\n    p.preprocess(data)\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/albumentations/core/utils.py\", line 83, in preprocess\n    data[data_name] = self.check_and_convert(data[data_name], rows, cols, direction=\"to\")\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/albumentations/core/utils.py\", line 91, in check_and_convert\n    return self.convert_to_albumentations(data, rows, cols)\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/albumentations/core/bbox_utils.py\", line 142, in convert_to_albumentations\n    return convert_bboxes_to_albumentations(data, self.params.format, rows, cols, check_validity=True)\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/albumentations/core/bbox_utils.py\", line 408, in convert_bboxes_to_albumentations\n    return [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) for bbox in bboxes]\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/albumentations/core/bbox_utils.py\", line 408, in <listcomp>\n    return [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) for bbox in bboxes]\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/albumentations/core/bbox_utils.py\", line 352, in convert_bbox_to_albumentations\n    check_bbox(bbox)\n  File \"/zhome/7c/2/136917/Desktop/venv_1/lib/python3.10/site-packages/albumentations/core/bbox_utils.py\", line 435, in check_bbox\n    raise ValueError(f\"Expected {name} for bbox {bbox} to be in the range [0.0, 1.0], got {value}.\")\nValueError: Expected y_max for bbox (tensor(0.4004), tensor(0.6511), tensor(0.9601), tensor(1.3481), tensor(1)) to be in the range [0.0, 1.0], got 1.348055124282837.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    print(f\"EPOCH {epoch}/{NUM_EPOCHS}\")\n",
    "\n",
    "    train_loss = []\n",
    "    train_correct = 0\n",
    "    train_len = 0\n",
    "    \n",
    "    for minibatch_no, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        \n",
    "        images = [image for image, _, _ in batch]\n",
    "        bboxes = [bbox for _, bbox, _ in batch]\n",
    "        labels = [label for _, _, label in batch]\n",
    "        \n",
    "        # Selective search\n",
    "        \n",
    "        cropped_images_all, proposals_all, predictions_all = selective_search_train(images, bboxes)            \n",
    "        data, target = torch.stack(cropped_images_all).to(device), torch.FloatTensor(predictions_all).to(device)\n",
    "        \n",
    "        # CNN\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)[:,0]\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        predicted = output > 0.5\n",
    "        train_correct += (target==predicted).sum().cpu().item()\n",
    "        train_len += data.shape[0]\n",
    "        print(f'train_loss: {loss:.5f}')\n",
    "        # break\n",
    "        \n",
    "    # Test evaluation\n",
    "    model.eval()\n",
    "    for batch in val_loader:\n",
    "        test_images = [image for image, _, _ in batch]\n",
    "        test_bboxes = [bbox for _, bbox, _ in batch]\n",
    "        test_labels = [label for _, _, label in batch]\n",
    "        \n",
    "        # Selective search\n",
    "        test_cropped_images_all, test_proposals_all = selective_search_test(test_images, test_bboxes) \n",
    "        test_data = torch.stack(test_cropped_images_all).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(test_data)[:,0]\n",
    "        predicted = (outputs > 0.5).tolist()\n",
    "        \n",
    "        # Reshaping\n",
    "        outputs = outputs.tolist()\n",
    "        new_shape = [len(l) for l in test_proposals_all]\n",
    "        output_new_shape, predicted_new_shape = [], []\n",
    "        head = 0\n",
    "        for l in new_shape:\n",
    "            output_new_shape.append(outputs[head:l+head])\n",
    "            predicted_new_shape.append(predicted[head:l+head])\n",
    "            head += l\n",
    "\n",
    "        # Filitering classes from background\n",
    "        predicted_bboxes = list(compress(test_proposals_all, predicted_new_shape))\n",
    "        output_new_shape = list(compress(output_new_shape, predicted_new_shape))\n",
    "        \n",
    "        pred = [dict(\n",
    "            boxes=torch.FloatTensor(bboxes),\n",
    "            scores=torch.FloatTensor(output),\n",
    "            labels=torch.ones(len(output)) # Simplification for Binary\n",
    "        ) for bboxes, output in zip(predicted_bboxes, output_new_shape)]\n",
    "        \n",
    "        target = [dict(\n",
    "            boxes=torch.FloatTensor(bboxes),\n",
    "            labels=torch.FloatTensor(label)\n",
    "        ) for bboxes, label in zip(test_bboxes, test_labels)]\n",
    "        \n",
    "        # Computing mAP\n",
    "        metric = MeanAveragePrecision()\n",
    "        metric.update(pred, target)\n",
    "        maps = metric.compute()\n",
    "        print(f'MAP: {float(maps[\"map\"].detach().cpu()):.3f}          MAP@50: {float(maps[\"map_50\"].detach().cpu()):.3f}          MAP_small: {float(maps[\"map_small\"].detach().cpu()):.3f}          MAP_large: {float(maps[\"map_large\"].detach().cpu()):.3f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41f4f2-d34c-47c7-942d-cc1148cebd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images = [image for image, _, _ in batch]\n",
    "bboxes = [bbox for _, bbox, _ in batch]\n",
    "labels = [label for _, _, label in batch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64446b0-0ffb-46d4-bea6-bff31afb5d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, bboxes, labels = trainset.__getitem__(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54188ffa-469d-472a-b552-b42190b12b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[1].detach().permute(1,2,0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd1f31-8af2-445c-965b-8232070cb4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1)\n",
    "\n",
    "plt.imshow(images[1].permute(1,2,0))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "# Show annotations\n",
    "for ann in bboxes[1]:\n",
    "    [x, y, x1, y1] = ann\n",
    "    rect = Rectangle((x,y),x1-x,y1-y,linewidth=2,edgecolor='red',\n",
    "                     facecolor='none', alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "#plt.savefig(f'imgs/output_{img_idx}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b1c0b7-3ab7-4a27-b4b7-7a0d60a3bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Selective search\n",
    "cropped_images_all, proposals_all, predictions_all = selective_search_train([images[1]], [bboxes[1]])            \n",
    "data, target = torch.stack(cropped_images_all).to(device), torch.FloatTensor(predictions_all).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04260dc8-e248-4918-aacd-242cd7dcd47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cropped_images_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d7deeb-af8e-4e02-a39b-7a46214ce2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da1b52-9ece-4fd4-a693-5955ae992197",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1)\n",
    "\n",
    "plt.imshow(images[1].permute(1,2,0))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "# Show annotations\n",
    "for i, ann in enumerate(proposals_all[0]):\n",
    "    \n",
    "    [x, y, x1, y1] = ann\n",
    "    if predictions_all[i]:\n",
    "        edge_col = 'green'\n",
    "    else:\n",
    "        edge_col = 'red'\n",
    "    rect = Rectangle((x,y),x1-x,y1-y,linewidth=2,edgecolor=edge_col,\n",
    "                     facecolor='none', alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "#plt.savefig(f'imgs/output_{img_idx}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a40bf4-8934-4f12-8d22-e149b958a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed094769-e562-4e5a-8501-37ee044a440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238a909-c0d0-49b4-a4c4-3e526ece5d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd86e3a-9c7a-47cc-9489-d1a06d2282e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3dfcda-465e-49b2-9754-daf8de9243ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data[3].detach().permute(1,2,0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acfbf59-c75e-4603-b442-be843f616324",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CNN\n",
    "optimizer.zero_grad()\n",
    "output = model(data)[:,0]\n",
    "loss = loss_function(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "train_loss.append(loss.item())\n",
    "predicted = output > 0.5\n",
    "train_correct += (target==predicted).sum().cpu().item()\n",
    "train_len += data.shape[0]\n",
    "print(f'train_loss: {loss:.5f}')\n",
    "# break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce0df94-000a-47ce-942e-8022058e5945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52f38dc-5725-4ad5-8721-25ec73827ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
