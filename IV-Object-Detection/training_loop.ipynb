{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "# pip install torchsummary\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as fn\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "from matplotlib.patches import Polygon, Rectangle\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "# Own imports \n",
    "from config import * \n",
    "from utils import *\n",
    "from data_loader import TacoDataset\n",
    "from eval import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c4e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=30, p=0.7),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.RGBShift(10, 10, 10, p=0.3),\n",
    "    A.GaussNoise(p=0.5),\n",
    "    A.Normalize(), # If you want to visualize - comment this line \n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',\n",
    "                            label_fields=['labels'],\n",
    "                            min_visibility=0.3, # min visibility of the original area in case of a crop\n",
    "                           )\n",
    ")\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',\n",
    "                            label_fields=['labels'],\n",
    "                           )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de156f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TacoDataset( 'train', transforms=train_transform, test_size=0.2) # test_transform for no augment\n",
    "valset   = TacoDataset('val', transforms=test_transform, test_size=0.2)\n",
    "testset  = TacoDataset('test', transforms=test_transform, test_size=0.2)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8,collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)\n",
    "val_loader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8,collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)\n",
    "test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8,collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bcbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_search(img):\n",
    "    \"\"\"\n",
    "    Takes image as an input (np.array not Tensor!)\n",
    "    Returns np.array (number of bboxes x 4)\n",
    "    Bboxes in format x, y, w, h (see demo notebook for example)\n",
    "    \"\"\"\n",
    "    # create selective search segmentation object\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    ss.setBaseImage(img) \n",
    "    # Choose between fast or accurate selective Search method: fast but low recall V.S. high recall but slow \n",
    "    ss.switchToSelectiveSearchFast()\n",
    "    # AM: Quality takes a looong time, maybe better to try with fast for now and see the results, if bad then change to quality\n",
    "    # ss.switchToSelectiveSearchQuality() \n",
    "    # run selective search\n",
    "    rects = ss.process()\n",
    "    print('Total Number of Region Proposals: {}'.format(len(rects))) # TODO: comment out after making the whole trainset work\n",
    "    return rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_model_set(model, freeze_convs=False,):\n",
    "    \n",
    "    if freeze_convs:\n",
    "        print('Freezing Convs')\n",
    "        # freeze the feature extractors\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    if type(model) == models.densenet.DenseNet:\n",
    "        in_features = model.classifier.in_features\n",
    "    \n",
    "    elif type(model) == models.resnet.ResNet:\n",
    "        in_features = model.fc.in_features\n",
    "    \n",
    "    \n",
    "    size_hidden = 512\n",
    "    out_features = 1\n",
    "    \n",
    "    head = nn.Sequential(\n",
    "                    nn.Linear(in_features, size_hidden),\n",
    "                    nn.Dropout(DROP_OUT_RATE),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm1d(size_hidden),\n",
    "                    nn.Linear(size_hidden, out_features),\n",
    "                    nn.Sigmoid()        \n",
    "    )\n",
    "                    \n",
    "    \n",
    "    if type(model) == models.densenet.DenseNet:\n",
    "        model.classifier = head\n",
    "    \n",
    "    elif type(model) == models.resnet.ResNet:\n",
    "        model.fc = head\n",
    "\n",
    "    else:\n",
    "        raise Exception('Not implemented the classifier for this type of model')\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "model = transfer_model_set(model, freeze_convs=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0.5 # Threshold for classification\n",
    "p = 0.05 # Probability of cropping background image\n",
    "\n",
    "def train(model, train_loader, test_loader, loss_function, optimizer, num_epochs, model_name, lr_scheduler=None, save_model=False ):\n",
    "    \n",
    "#     def loss_fun(output, target):\n",
    "#         return F.cross_entropy(output, target)\n",
    "    \n",
    "    out_dict = {'train_acc': [],\n",
    "                'test_acc': [],\n",
    "                'train_loss': [],\n",
    "                'test_loss': []}\n",
    "  \n",
    "    for epoch in tqdm(range(num_epochs), unit='epoch'):\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_len = 0\n",
    "        train_loss = []\n",
    "        for minibatch_no, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            images = [image for image, _, _ in batch]\n",
    "            bboxes = [bbox for _, bbox, _ in batch]\n",
    "            labels = [label for _, _, label in batch]\n",
    "            # images, bboxes, labels = torch.FloatTensor(images).to(device), torch.FloatTensor(bboxes).to(device), torch.FloatTensor(labels).to(device)\n",
    "            # print(images.shape)\n",
    "            \n",
    "            # Selective search\n",
    "            proposals_all = []\n",
    "            predictions_all = []\n",
    "            cropped_images_all = []\n",
    "            for image, img_bboxes in zip(images, bboxes):\n",
    "                proposals = selective_search(image.permute([1,2,0]).numpy()) # .cpu()\n",
    "                proposals_all.append(proposals)\n",
    "                \n",
    "                # IoU\n",
    "                for proposal in proposals:\n",
    "                    scores_all = []\n",
    "                    for bbox in img_bboxes:\n",
    "                        score = IoU(proposal, bbox)\n",
    "                        scores_all.append(score)\n",
    "                    \n",
    "                    prediction = max(scores_all) > k\n",
    "                    \n",
    "                    # Extract image\n",
    "                    if prediction or random.random() < p:\n",
    "                        cropped_image = fn.crop(image, *proposal)\n",
    "                        resized_image = fn.resize(cropped_image, size=[1024, 1024])\n",
    "                        cropped_images_all.append(resized_image)\n",
    "                        predictions_all.append(prediction)\n",
    "                        \n",
    "            # print(f\"Len: {len(predictions_all)}, sum: {sum(predictions_all)}\")\n",
    "            \n",
    "            data, target = torch.stack(cropped_images_all).to(device), torch.FloatTensor(predictions_all).to(device)\n",
    "            \n",
    "            \n",
    "            # CNN\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(data)[:,0]\n",
    "#             loss = loss_function(output, target)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             if lr_scheduler is not None:\n",
    "#                 lr_scheduler.step()\n",
    "#             train_loss.append(loss.item())\n",
    "#             predicted = output > 0.5\n",
    "#             train_correct += (target==predicted).sum().cpu().item()\n",
    "#             train_len += data.shape[0]\n",
    "            \n",
    "#         test_loss = []\n",
    "#         test_correct = 0\n",
    "#         test_len = 0\n",
    "#         model.eval()\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(torch.float32).to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 output = model(data)[:,0]\n",
    "#             test_loss.append(loss_function(output, target).cpu().item())\n",
    "#             predicted = output > 0.5\n",
    "#             test_correct += (target==predicted).sum().cpu().item()\n",
    "#             test_len += data.shape[0]\n",
    "\n",
    "#         if save_model and epoch > 0 and test_correct/test_len > max(out_dict['test_acc']):\n",
    "#             torch.save(model, 'models/' + model_name)\n",
    "            \n",
    "            \n",
    "#         out_dict['train_acc'].append(train_correct/train_len)\n",
    "#         out_dict['test_acc'].append(test_correct/test_len)\n",
    "#         out_dict['train_loss'].append(np.mean(train_loss))\n",
    "#         out_dict['test_loss'].append(np.mean(test_loss))\n",
    "\n",
    "        \n",
    "#         print(f\"Loss train: {np.mean(train_loss):.3f}\\t test: {np.mean(test_loss):.3f}\\t\",\n",
    "#               f\"Accuracy train: {out_dict['train_acc'][-1]*100:.1f}%\\t test: {out_dict['test_acc'][-1]*100:.1f}%\")\n",
    "        \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2eaef8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_dict = train(model,\n",
    "                          train_loader,\n",
    "                          test_loader,\n",
    "                          LOSS_FN,\n",
    "                          optimizer,\n",
    "                          EPOCHS, \n",
    "                          save_model=True, \n",
    "                          model_name='densenet121_full_Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7038041",
   "metadata": {},
   "outputs": [],
   "source": [
    "for it, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "    batch = data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b2df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [image for image, _, _ in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e27700",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(images).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
