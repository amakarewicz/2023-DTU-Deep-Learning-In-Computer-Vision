{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7d6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "# pip install torchsummary\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "from matplotlib.patches import Polygon, Rectangle\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "# Own imports \n",
    "from config import * \n",
    "from utils import *\n",
    "from data_loader import TacoDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0c4e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 1024\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(img_size, img_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=30, p=0.7),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.RGBShift(10, 10, 10, p=0.3),\n",
    "    A.GaussNoise(p=0.5),\n",
    "    A.Normalize(), # If you want to visualize - comment this line \n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',\n",
    "                            label_fields=['labels'],\n",
    "                            min_visibility=0.3, # min visibility of the original area in case of a crop\n",
    "                           )\n",
    ")\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(img_size, img_size),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',\n",
    "                            label_fields=['labels'],\n",
    "                           )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de156f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TacoDataset( 'train', transforms=train_transform, test_size=0.2) # test_transform for no augment\n",
    "valset   = TacoDataset('val', transforms=test_transform, test_size=0.2)\n",
    "testset  = TacoDataset('test', transforms=test_transform, test_size=0.2)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8,collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)\n",
    "val_loader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8,collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)\n",
    "test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8,collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46b8a56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.3652,  0.7933,  1.0673,  ...,  0.0398, -0.3198, -0.5596],\n",
       "          [ 0.0398,  0.0741,  0.2111,  ...,  0.0569, -0.0458, -0.5424],\n",
       "          [-0.7822, -0.9363, -0.5082,  ...,  0.0569, -0.0458, -0.3198],\n",
       "          ...,\n",
       "          [-0.3198, -0.3541, -0.2684,  ..., -0.1828, -0.2342, -0.2856],\n",
       "          [-0.0458, -0.6794, -0.5596,  ..., -0.0972, -0.1828, -0.2342],\n",
       "          [ 0.1939, -1.0219, -1.1932,  ..., -0.0287, -0.3712, -0.3883]],\n",
       " \n",
       "         [[ 0.9755,  1.4657,  1.7458,  ...,  0.9755,  0.5203,  0.1702],\n",
       "          [ 0.5728,  0.7129,  0.8880,  ...,  1.1155,  0.9580,  0.4153],\n",
       "          [-0.3200, -0.3725,  0.1352,  ...,  1.1856,  1.0980,  0.8004],\n",
       "          ...,\n",
       "          [ 0.5028,  0.4678,  0.5553,  ..., -0.2850, -0.3550, -0.4251],\n",
       "          [ 0.8179,  0.1527,  0.2577,  ..., -0.2500, -0.3550, -0.3901],\n",
       "          [ 1.0280, -0.3200, -0.5476,  ..., -0.3200, -0.6352, -0.5826]],\n",
       " \n",
       "         [[-0.0615,  0.4265,  0.7054,  ...,  1.3154,  0.8971,  0.5311],\n",
       "          [-0.2358, -0.1661,  0.0082,  ...,  1.4722,  1.3328,  0.7751],\n",
       "          [-1.1073, -1.2119, -0.7238,  ...,  1.5420,  1.4897,  1.2108],\n",
       "          ...,\n",
       "          [-0.6890, -0.7064, -0.5844,  ...,  0.2173,  0.1302,  0.0431],\n",
       "          [-0.5670, -1.0550, -0.8284,  ...,  0.2871,  0.1651,  0.0779],\n",
       "          [-0.3753, -1.3164, -1.2641,  ...,  0.2871, -0.0615, -0.1138]]]),\n",
       " [(470.6512756347656, 406.3779296875, 618.384033203125, 499.1621398925781),\n",
       "  (791.817138671875,\n",
       "   62.01861572265625,\n",
       "   869.4114379882812,\n",
       "   106.08587646484375)],\n",
       " [tensor(1), tensor(1)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4de27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# def train(model, train_loader, test_loader, loss_function, optimizer, num_epochs, model_name, lr_scheduler=None, save_model=False ):\n",
    "    \n",
    "# #     def loss_fun(output, target):\n",
    "# #         return F.cross_entropy(output, target)\n",
    "    \n",
    "#     out_dict = {'train_acc': [],\n",
    "#                 'test_acc': [],\n",
    "#                 'train_loss': [],\n",
    "#                 'test_loss': []}\n",
    "  \n",
    "#     for epoch in tqdm(range(num_epochs), unit='epoch'):\n",
    "#         model.train()\n",
    "#         train_correct = 0\n",
    "#         train_len = 0\n",
    "#         train_loss = []\n",
    "#         for minibatch_no, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "#             data, target = data.to(device), target.to(torch.float32).to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(data)[:,0]\n",
    "#             loss = loss_function(output, target)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             if lr_scheduler is not None:\n",
    "#                 lr_scheduler.step()\n",
    "#             train_loss.append(loss.item())\n",
    "#             predicted = output > 0.5\n",
    "#             train_correct += (target==predicted).sum().cpu().item()\n",
    "#             train_len += data.shape[0]\n",
    "            \n",
    "#         test_loss = []\n",
    "#         test_correct = 0\n",
    "#         test_len = 0\n",
    "#         model.eval()\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(torch.float32).to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 output = model(data)[:,0]\n",
    "#             test_loss.append(loss_function(output, target).cpu().item())\n",
    "#             predicted = output > 0.5\n",
    "#             test_correct += (target==predicted).sum().cpu().item()\n",
    "#             test_len += data.shape[0]\n",
    "\n",
    "#         if save_model and epoch > 0 and test_correct/test_len > max(out_dict['test_acc']):\n",
    "#             torch.save(model, 'models/' + model_name)\n",
    "            \n",
    "            \n",
    "#         out_dict['train_acc'].append(train_correct/train_len)\n",
    "#         out_dict['test_acc'].append(test_correct/test_len)\n",
    "#         out_dict['train_loss'].append(np.mean(train_loss))\n",
    "#         out_dict['test_loss'].append(np.mean(test_loss))\n",
    "\n",
    "        \n",
    "#         print(f\"Loss train: {np.mean(train_loss):.3f}\\t test: {np.mean(test_loss):.3f}\\t\",\n",
    "#               f\"Accuracy train: {out_dict['train_acc'][-1]*100:.1f}%\\t test: {out_dict['test_acc'][-1]*100:.1f}%\")\n",
    "        \n",
    "#     return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20a8d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# def transfer_model_set(model, freeze_convs=False,):\n",
    "    \n",
    "#     if freeze_convs:\n",
    "#         print('Freezing Convs')\n",
    "#         # freeze the feature extractors\n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = False\n",
    "    \n",
    "#     if type(model) == models.densenet.DenseNet:\n",
    "#         in_features = model.classifier.in_features\n",
    "    \n",
    "#     elif type(model) == models.resnet.ResNet:\n",
    "#         in_features = model.fc.in_features\n",
    "    \n",
    "    \n",
    "#     size_hidden = 512\n",
    "#     out_features = 1\n",
    "    \n",
    "#     head = nn.Sequential(\n",
    "#                     nn.Linear(in_features, size_hidden),\n",
    "#                     nn.Dropout(DROP_OUT_RATE),\n",
    "#                     nn.ReLU(),\n",
    "#                     nn.BatchNorm1d(size_hidden),\n",
    "#                     nn.Linear(size_hidden, out_features),\n",
    "#                     nn.Sigmoid()        \n",
    "#     )\n",
    "                    \n",
    "    \n",
    "#     if type(model) == models.densenet.DenseNet:\n",
    "#         model.classifier = head\n",
    "    \n",
    "#     elif type(model) == models.resnet.ResNet:\n",
    "#         model.fc = head\n",
    "\n",
    "#     else:\n",
    "#         raise Exception('Not implemented the classifier for this type of model')\n",
    "\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f2eaef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# HEAD_LEARNING_RATE = 0.001\n",
    "# NUM_EPOCHS = 5\n",
    "# loss = nn.BCELoss()\n",
    "# DROP_OUT_RATE = 0.5\n",
    "\n",
    "# model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "# model = transfer_model_set(model, freeze_convs=False)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), HEAD_LEARNING_RATE)\n",
    "\n",
    "# out_dict = train(model,\n",
    "#                           train_loader,\n",
    "#                           test_loader,\n",
    "#                           loss,\n",
    "#                           optimizer,\n",
    "#                           NUM_EPOCHS, \n",
    "#                           save_model=True, \n",
    "#                           model_name='densenet121_full_Adam')\n",
    "\n",
    "# # saving results\n",
    "# d = out_dict\n",
    "# # with open(f'results/results_transfer_densenet_full_{NUM_EPOCHS}_epochs_{FULL_LEARNING_RATE:.0e}_lr_{optim}_optim.csv', 'w') as csvFile:\n",
    "# #     writer = csv.writer(csvFile)\n",
    "# #     writer.writerow(d.keys())\n",
    "# #     writer.writerows(zip(*d.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
