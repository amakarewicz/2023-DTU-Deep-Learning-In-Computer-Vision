{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ffd46c-e3db-457c-b762-750d061a5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7d6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "# pip install torchsummary\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as fn\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "from matplotlib.patches import Polygon, Rectangle\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "# Own imports \n",
    "from config import * \n",
    "from utils import *\n",
    "from data_loader import TacoDataset\n",
    "from eval import *\n",
    "\n",
    "# speed-up using multithreads\n",
    "cv2.setUseOptimized(True);\n",
    "cv2.setNumThreads(8);\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c4e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 512\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(img_size, img_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=30, p=0.7),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.RGBShift(10, 10, 10, p=0.3),\n",
    "    A.GaussNoise(p=0.5),\n",
    "    A.Normalize(), # If you want to visualize - comment this line \n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='coco',\n",
    "                            label_fields=['labels'],\n",
    "                            min_visibility=0.3, # min visibility of the original area in case of a crop\n",
    "                           )\n",
    ")\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(img_size, img_size),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='coco',\n",
    "                            label_fields=['labels'],\n",
    "                           )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a040fe49-393f-44eb-bdfd-f0a5d9110564",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de156f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TacoDataset( 'train', transforms=train_transform, test_size=0.2) # test_transform for no augment\n",
    "valset   = TacoDataset('val', transforms=test_transform, test_size=0.2)\n",
    "testset  = TacoDataset('test', transforms=test_transform, test_size=0.2)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=12, collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)\n",
    "val_loader = DataLoader(valset, batch_size=1, shuffle=False, num_workers=12, collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)\n",
    "test_loader = DataLoader(testset, batch_size=1, shuffle=False, num_workers=12, collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8513dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "\n",
    "model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "model = transfer_model_set(model, freeze_convs=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), HEAD_LEARNING_RATE)\n",
    "\n",
    "lr_scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef75c06c-0cfb-4319-92f4-271666a7a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72397995-c9c9-40bb-83a8-22dea7a6cef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff27311bc5a40ca8bd3eebea16b20fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/8d/e/198218/dlincv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    print(f\"EPOCH {epoch}/{NUM_EPOCHS}\")\n",
    "\n",
    "    train_loss = []\n",
    "    train_correct = 0\n",
    "    train_len = 0\n",
    "    \n",
    "    for minibatch_no, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        \n",
    "        images = [image for image, _, _ in batch]\n",
    "        bboxes = [bbox for _, bbox, _ in batch]\n",
    "        labels = [label for _, _, label in batch]\n",
    "        \n",
    "        # Selective search\n",
    "                \n",
    "        # Edge boxes\n",
    "        cropped_images_all, proposals_all, predictions_all = edge_boxes_train(images, bboxes)         \n",
    "        if len(cropped_images_all) ==0:\n",
    "            print('no boxes detected')\n",
    "            continue \n",
    "        data, target = torch.stack(cropped_images_all).to(device), torch.FloatTensor(predictions_all).to(device)\n",
    "        \n",
    "        # CNN\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)[:,0]\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        predicted = output > 0.5\n",
    "        correct_in_the_item = (target==predicted).sum().cpu().item()\n",
    "        train_correct += correct_in_the_item\n",
    "        train_len += data.shape[0]\n",
    "        print(f'train_loss: {loss:.5f}        Accuracy {correct_in_the_item / len(target):.3f}')\n",
    "\n",
    "        # for now we break\n",
    "\n",
    "        break \n",
    "        \n",
    "    # Test evaluation\n",
    "    model.eval()\n",
    "    for batch in val_loader: # Keep loader at 1 \n",
    "        test_images, test_bboxes, test_labels = batch[0]\n",
    "        \n",
    "        # Selective search\n",
    "        print('selecting boxes...')\n",
    "        test_cropped_images_all, test_proposals_all = edge_boxes_test(test_images, test_bboxes) \n",
    "        test_data = torch.stack(test_cropped_images_all).to(device)\n",
    "        \n",
    "        print('predicting...')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(test_data)[:,0]\n",
    "        \n",
    "        bboxes_xywh = torch.stack(test_proposals_all).to(device).to(float)\n",
    "        bboxes_xyxy = copy.deepcopy(bboxes_xywh)\n",
    "        bboxes_xyxy[:, 2] = bboxes_xyxy[:, 2] + bboxes_xyxy[:, 0]\n",
    "        bboxes_xyxy[:, 3] = bboxes_xyxy[:, 3] + bboxes_xyxy[:, 1]\n",
    "\n",
    "        print('non max supr')\n",
    "        \n",
    "        bboxes_indices = torchvision.ops.nms(bboxes_xyxy, outputs, iou_threshold=0.1)\n",
    "        \n",
    "        final_bboxes = bboxes_xywh[bboxes_indices]\n",
    "        outputs = outputs[bboxes_indices]\n",
    "        \n",
    "        # Reshaping\n",
    "        outputs = outputs.tolist()\n",
    "        \n",
    "        pred = [dict(\n",
    "            boxes=torch.FloatTensor(bboxes),\n",
    "            scores=torch.FloatTensor(scores),\n",
    "            labels=torch.FloatTensor(np.ones(len(scores))) # Simplification for Binary\n",
    "        ) for bboxes, scores in zip(final_bboxes, outputs)]\n",
    "        \n",
    "        target = [dict(\n",
    "            boxes=torch.FloatTensor(bboxes),\n",
    "            labels=torch.FloatTensor(test_target)\n",
    "        ) for bboxes, label in zip(test_bboxes, test_labels)]\n",
    "        \n",
    "        # Computing mAP\n",
    "        metric = MeanAveragePrecision()\n",
    "        metric.update(pred, target)\n",
    "        maps = metric.compute()\n",
    "        print(f'MAP: {float(maps[\"map\"].detach().cpu()):.3f}          MAP@50: {float(maps[\"map_50\"].detach().cpu()):.3f}          MAP_small: {float(maps[\"map_small\"].detach().cpu()):.3f}          MAP_large: {float(maps[\"map_large\"].detach().cpu()):.3f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f7a6e0-eb0b-43b3-9ce6-8782227ea689",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_loader:\n",
    "        test_images, test_bboxes, test_labels = batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c9d05-03ab-4092-9357-0c3f9dcd6dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cropped_images_all, test_proposals_all = edge_boxes_test(test_images, test_bboxes) \n",
    "\n",
    "\n",
    "# Reshaping\n",
    "#outputs = outputs.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e89645-4ae2-42c8-97d6-90c558bdd0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_xywh = torch.stack(test_proposals_all).to(device).to(float)\n",
    "bboxes_xyxy = copy.deepcopy(bboxes_xywh)\n",
    "bboxes_xyxy[:, 2] = bboxes_xyxy[:, 2] + bboxes_xyxy[:, 0]\n",
    "bboxes_xyxy[:, 3] = bboxes_xyxy[:, 3] + bboxes_xyxy[:, 1]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823196b6-9232-48d2-b835-1d2315550a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes_indices = torchvision.ops.nms(bboxes_xyxy, outputs, iou_threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d2540-e511-4ae9-877c-450b29dfda49",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bboxes = bboxes_xywh[bboxes_indices]\n",
    "outputs = outputs[bboxes_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3525ad6f-94a7-42eb-add6-d6379a0bf326",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb1f421-4fcc-448b-a0b1-c355e9b10463",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = outputs[bboxes_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f21c5-9373-46e9-9b5e-ca3a8d2580a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1)\n",
    "img = test_images.detach().cpu()\n",
    "\n",
    "img = denormalize(img)\n",
    "\n",
    "plt.imshow(img.detach().cpu().permute(1,2,0))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "# Show annotations\n",
    "for i, ann in enumerate(final_bboxes.cpu().numpy()):\n",
    "    \n",
    "    [x, y, x1, y1] = ann\n",
    "    if predictions_all[i]:\n",
    "        edge_col = 'green'\n",
    "    else:\n",
    "        edge_col = 'red'\n",
    "    rect = Rectangle((x,y),x1,y1,linewidth=2,edgecolor=edge_col,\n",
    "                     facecolor='none', alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "#plt.savefig(f'imgs/output_{img_idx}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b510909-4437-4bea-a35b-70fbaf04f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = np.array(outputs)[predicted]\n",
    "test_proposals = np.array(test_proposals_all)[predicted]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b25a4-eee5-4bb1-8526-e1b4d6289e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test_proposals_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c184f-4923-4d05-83b0-030a3bd2aeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = torch.stack(cropped_images_all).to(device), torch.FloatTensor(predictions_all).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d938ec-b631-4eec-a692-b9810a03e831",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac7a12-f1d8-430e-9ca5-8334d9814890",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6cd69-64ff-4472-b576-a4bdf3930607",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(data)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30d0324-540d-4b6c-b418-a1b5daf2269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1988e5ab-c08a-46c6-ad7b-610c5a94cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_data)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecd54b6-9183-4113-9d74-1663cb89f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = output > 0.5\n",
    "train_correct += (target==predicted).sum().cpu().item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafb178-f3e2-4fde-b857-e022c6d46d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "(target==predicted).sum().cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e197d5-9e10-453e-8b78-ff4fd7e1701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41f4f2-d34c-47c7-942d-cc1148cebd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [image for image, _, _ in batch]\n",
    "bboxes = [bbox for _, bbox, _ in batch]\n",
    "labels = [label for _, _, label in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54188ffa-469d-472a-b552-b42190b12b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "cropped_images_all, proposals_all, predictions_all = edge_boxes_train([images[k]], [bboxes[k]])       \n",
    "img = images[k]##.cpu().numpy()\n",
    "\n",
    "#plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed67df-a4e6-4d68-b353-ca1c74479c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(img):\n",
    "    mean=[0.485, 0.456, 0.406]\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "    \n",
    "    denormalize = transforms.Normalize(mean=[-0.485, -0.456, -0.406], \n",
    "                         std=[1/0.229, 1/0.224, 1/0.225])\n",
    "\n",
    "    denorm_image = denormalize(img)\n",
    "    x = ((denorm_image - denorm_image.min())/(denorm_image - denorm_image.min()).max() * 255).to(torch.int64)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce92bc-623b-4c8f-ad0c-a6f52673aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = denormalize(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb7020-baf7-4c06-a3d2-6f787f5d16d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd1f31-8af2-445c-965b-8232070cb4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1)\n",
    "\n",
    "plt.imshow(img.permute(1,2,0))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "# Show annotations\n",
    "for ann in bboxes[k]:\n",
    "    [x, y, x1, y1] = ann\n",
    "    rect = Rectangle((x,y),x1,y1,linewidth=2,edgecolor='green',\n",
    "                     facecolor='none', alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "#plt.savefig(f'imgs/output_{img_idx}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da1b52-9ece-4fd4-a693-5955ae992197",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1)\n",
    "\n",
    "plt.imshow(img.permute(1,2,0))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "# Show annotations\n",
    "for i, ann in enumerate(proposals_all[0]):\n",
    "    \n",
    "    [x, y, x1, y1] = ann\n",
    "    if predictions_all[i]:\n",
    "        edge_col = 'green'\n",
    "    else:\n",
    "        edge_col = 'red'\n",
    "    rect = Rectangle((x,y),x1,y1,linewidth=2,edgecolor=edge_col,\n",
    "                     facecolor='none', alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "#plt.savefig(f'imgs/output_{img_idx}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768bb54-827f-4018-b809-82fa23e55e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = torch.stack(cropped_images_all).to(device), torch.FloatTensor(predictions_all).to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c5c10-7cbd-460f-96f8-d93870b41833",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[k].permute(1,2,0).detach().cpu().numpy()[y:y+h, x:x+w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987d35e-a8e4-404c-8681-88be582eb24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fn.crop(, x, y, w, h ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1afad6-ebc2-4f9a-a6f6-79ce6caed759",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da699d-65f3-47c2-9e0d-c44443e4d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cropped_images_all[1].detach().permute(1,2,0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5115e8d-2643-41ef-bc03-f656d5062628",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0aa5a4-28d2-46b0-a758-68424c1cc7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3dfcda-465e-49b2-9754-daf8de9243ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_data[3].detach().permute(1,2,0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acfbf59-c75e-4603-b442-be843f616324",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CNN\n",
    "optimizer.zero_grad()\n",
    "output = model(data)[:,0]\n",
    "loss = loss_function(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "train_loss.append(loss.item())\n",
    "predicted = output > 0.5\n",
    "train_correct += (target==predicted).sum().cpu().item()\n",
    "train_len += data.shape[0]\n",
    "print(f'train_loss: {loss:.5f}')\n",
    "# break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce0df94-000a-47ce-942e-8022058e5945",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "output = model(data)[:,0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52f38dc-5725-4ad5-8721-25ec73827ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
