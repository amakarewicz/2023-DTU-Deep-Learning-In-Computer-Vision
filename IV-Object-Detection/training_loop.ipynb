{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ffd46c-e3db-457c-b762-750d061a5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b7d6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "# pip install torchsummary\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as fn\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import csv\n",
    "\n",
    "from matplotlib.patches import Polygon, Rectangle\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "# Own imports \n",
    "from config import * \n",
    "from utils import *\n",
    "from data_loader import TacoDataset\n",
    "from eval import *\n",
    "\n",
    "# speed-up using multithreads\n",
    "cv2.setUseOptimized(True);\n",
    "cv2.setNumThreads(8);\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c4e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 512\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(img_size, img_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=30, p=0.7),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.RGBShift(10, 10, 10, p=0.3),\n",
    "    A.GaussNoise(p=0.5),\n",
    "    A.Normalize(), # If you want to visualize - comment this line \n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='coco',\n",
    "                            label_fields=['labels'],\n",
    "                            min_visibility=0.3, # min visibility of the original area in case of a crop\n",
    "                           )\n",
    ")\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(img_size, img_size),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='coco',\n",
    "                            label_fields=['labels'],\n",
    "                           )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a040fe49-393f-44eb-bdfd-f0a5d9110564",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de156f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TacoDataset( 'train', transforms=train_transform, test_size=0.2) # test_transform for no augment\n",
    "valset   = TacoDataset('val', transforms=test_transform, test_size=0.2)\n",
    "testset  = TacoDataset('test', transforms=test_transform, test_size=0.2)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=12, collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)\n",
    "val_loader = DataLoader(valset, batch_size=1, shuffle=False, num_workers=12, collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)\n",
    "test_loader = DataLoader(testset, batch_size=1, shuffle=False, num_workers=12, collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8513dc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /zhome/dc/f/181253/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 83.3M/83.3M [00:00<00:00, 177MB/s]\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "\n",
    "model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "model = transfer_model_set(model, freeze_convs=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), HEAD_LEARNING_RATE)\n",
    "\n",
    "lr_scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef75c06c-0cfb-4319-92f4-271666a7a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "EDGE_BOXES_IMG_SIZE_IN = img_size\n",
    "EDGE_BOXES_IMG_SIZE_OUT = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72397995-c9c9-40bb-83a8-22dea7a6cef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd34c517635459d8c489df8db287672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_127705/563744479.py:32: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  data, target = torch.stack(cropped_images_all).to(device), torch.FloatTensor(predictions_all).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.28786        Accuracy 0.893\n",
      "train_loss: 0.34695        Accuracy 0.857\n",
      "train_loss: 0.48315        Accuracy 0.786\n",
      "train_loss: 0.42447        Accuracy 0.821\n",
      "train_loss: 0.37799        Accuracy 0.857\n",
      "train_loss: 0.32364        Accuracy 0.860\n",
      "train_loss: 0.36827        Accuracy 0.867\n",
      "train_loss: 0.28735        Accuracy 0.907\n",
      "train_loss: 0.29295        Accuracy 0.906\n",
      "train_loss: 0.20849        Accuracy 0.923\n",
      "train_loss: 0.24810        Accuracy 0.927\n",
      "train_loss: 0.28536        Accuracy 0.900\n",
      "train_loss: 0.30670        Accuracy 0.889\n",
      "train_loss: 0.26814        Accuracy 0.894\n",
      "train_loss: 0.21246        Accuracy 0.933\n",
      "train_loss: 0.32941        Accuracy 0.844\n",
      "train_loss: 0.27817        Accuracy 0.883\n",
      "train_loss: 0.30539        Accuracy 0.858\n",
      "train_loss: 0.17382        Accuracy 0.951\n",
      "train_loss: 0.27983        Accuracy 0.882\n",
      "train_loss: 0.27821        Accuracy 0.882\n",
      "train_loss: 0.29764        Accuracy 0.907\n",
      "train_loss: 0.32002        Accuracy 0.859\n",
      "train_loss: 0.25825        Accuracy 0.908\n",
      "train_loss: 0.17813        Accuracy 0.931\n",
      "train_loss: 0.28095        Accuracy 0.886\n",
      "train_loss: 0.35449        Accuracy 0.892\n",
      "train_loss: 0.35347        Accuracy 0.829\n",
      "train_loss: 0.24313        Accuracy 0.908\n",
      "train_loss: 0.25818        Accuracy 0.903\n",
      "train_loss: 0.28955        Accuracy 0.873\n",
      "train_loss: 0.20439        Accuracy 0.898\n",
      "train_loss: 0.24908        Accuracy 0.887\n",
      "train_loss: 0.42746        Accuracy 0.832\n",
      "train_loss: 0.29230        Accuracy 0.906\n",
      "train_loss: 0.27514        Accuracy 0.876\n",
      "train_loss: 0.28505        Accuracy 0.878\n",
      "train_loss: 0.36031        Accuracy 0.876\n",
      "train_loss: 0.29657        Accuracy 0.886\n",
      "train_loss: 0.22018        Accuracy 0.906\n",
      "train_loss: 0.21956        Accuracy 0.927\n",
      "train_loss: 0.32763        Accuracy 0.858\n",
      "train_loss: 0.27344        Accuracy 0.881\n",
      "train_loss: 0.22057        Accuracy 0.915\n",
      "train_loss: 0.24292        Accuracy 0.916\n",
      "train_loss: 0.20027        Accuracy 0.918\n",
      "train_loss: 0.21811        Accuracy 0.931\n",
      "train_loss: 0.37742        Accuracy 0.878\n",
      "train_loss: 0.18358        Accuracy 0.946\n",
      "train_loss: 0.21816        Accuracy 0.908\n",
      "train_loss: 0.23856        Accuracy 0.914\n",
      "train_loss: 0.25810        Accuracy 0.888\n",
      "train_loss: 0.23418        Accuracy 0.921\n",
      "train_loss: 0.20275        Accuracy 0.943\n",
      "train_loss: 0.28279        Accuracy 0.870\n",
      "train_loss: 0.31992        Accuracy 0.856\n",
      "train_loss: 0.15201        Accuracy 0.946\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36d9526bc7c4fd193f75945e7dd9873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.077          MAP@50: 0.257          MAP_small: 0.000          MAP_large: -1.000\n",
      "EPOCH 1/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75df28e85cc7435f9f439eca1b90e4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.13698        Accuracy 0.948\n",
      "train_loss: 0.42969        Accuracy 0.826\n",
      "train_loss: 0.37019        Accuracy 0.852\n",
      "train_loss: 0.39819        Accuracy 0.838\n",
      "train_loss: 0.36805        Accuracy 0.849\n",
      "train_loss: 0.31596        Accuracy 0.872\n",
      "train_loss: 0.32378        Accuracy 0.845\n",
      "train_loss: 0.35034        Accuracy 0.838\n",
      "train_loss: 0.32885        Accuracy 0.857\n",
      "train_loss: 0.28346        Accuracy 0.890\n",
      "train_loss: 0.25062        Accuracy 0.906\n",
      "train_loss: 0.27295        Accuracy 0.898\n",
      "train_loss: 0.17781        Accuracy 0.954\n",
      "train_loss: 0.28077        Accuracy 0.848\n",
      "train_loss: 0.31771        Accuracy 0.839\n",
      "train_loss: 0.30871        Accuracy 0.867\n",
      "train_loss: 0.20269        Accuracy 0.934\n",
      "train_loss: 0.19856        Accuracy 0.931\n",
      "train_loss: 0.27359        Accuracy 0.894\n",
      "train_loss: 0.19687        Accuracy 0.936\n",
      "train_loss: 0.27734        Accuracy 0.878\n",
      "train_loss: 0.38679        Accuracy 0.856\n",
      "train_loss: 0.28276        Accuracy 0.899\n",
      "train_loss: 0.18782        Accuracy 0.939\n",
      "train_loss: 0.17102        Accuracy 0.945\n",
      "train_loss: 0.40974        Accuracy 0.846\n",
      "train_loss: 0.19616        Accuracy 0.935\n",
      "train_loss: 0.17848        Accuracy 0.923\n",
      "train_loss: 0.16488        Accuracy 0.938\n",
      "train_loss: 0.18844        Accuracy 0.940\n",
      "train_loss: 0.24655        Accuracy 0.873\n",
      "train_loss: 0.23636        Accuracy 0.928\n",
      "train_loss: 0.27855        Accuracy 0.909\n",
      "train_loss: 0.20181        Accuracy 0.944\n",
      "train_loss: 0.29905        Accuracy 0.891\n",
      "train_loss: 0.26992        Accuracy 0.867\n",
      "train_loss: 0.27417        Accuracy 0.908\n",
      "train_loss: 0.23404        Accuracy 0.908\n",
      "train_loss: 0.27384        Accuracy 0.890\n",
      "train_loss: 0.27384        Accuracy 0.888\n",
      "train_loss: 0.19669        Accuracy 0.912\n",
      "train_loss: 0.21473        Accuracy 0.909\n",
      "train_loss: 0.25647        Accuracy 0.917\n",
      "train_loss: 0.35059        Accuracy 0.861\n",
      "train_loss: 0.36001        Accuracy 0.858\n",
      "train_loss: 0.31027        Accuracy 0.864\n",
      "train_loss: 0.24993        Accuracy 0.889\n",
      "train_loss: 0.28907        Accuracy 0.875\n",
      "train_loss: 0.23650        Accuracy 0.918\n",
      "train_loss: 0.19807        Accuracy 0.928\n",
      "train_loss: 0.20241        Accuracy 0.941\n",
      "train_loss: 0.50607        Accuracy 0.795\n",
      "train_loss: 0.26543        Accuracy 0.895\n",
      "train_loss: 0.28053        Accuracy 0.903\n",
      "train_loss: 0.26712        Accuracy 0.901\n",
      "train_loss: 0.28699        Accuracy 0.880\n",
      "train_loss: 0.58862        Accuracy 0.765\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbce80f3af8f4ce484dd0c10c678f2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.000          MAP@50: 0.000          MAP_small: 0.000          MAP_large: -1.000\n",
      "EPOCH 2/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54447be6dbdf4bad9ff8ccfed57f11d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.22599        Accuracy 0.901\n",
      "train_loss: 0.28698        Accuracy 0.888\n",
      "train_loss: 0.34447        Accuracy 0.869\n",
      "train_loss: 0.22959        Accuracy 0.897\n",
      "train_loss: 0.26103        Accuracy 0.913\n",
      "train_loss: 0.29715        Accuracy 0.907\n",
      "train_loss: 0.23706        Accuracy 0.924\n",
      "train_loss: 0.30658        Accuracy 0.878\n",
      "train_loss: 0.29810        Accuracy 0.844\n",
      "train_loss: 0.29346        Accuracy 0.869\n",
      "train_loss: 0.25389        Accuracy 0.892\n",
      "train_loss: 0.34531        Accuracy 0.880\n",
      "train_loss: 0.17595        Accuracy 0.955\n",
      "train_loss: 0.27188        Accuracy 0.867\n",
      "train_loss: 0.27874        Accuracy 0.886\n",
      "train_loss: 0.26888        Accuracy 0.863\n",
      "train_loss: 0.21709        Accuracy 0.931\n",
      "train_loss: 0.27277        Accuracy 0.880\n",
      "train_loss: 0.26280        Accuracy 0.895\n",
      "train_loss: 0.24192        Accuracy 0.896\n",
      "train_loss: 0.29892        Accuracy 0.849\n",
      "train_loss: 0.25396        Accuracy 0.907\n",
      "train_loss: 0.31803        Accuracy 0.840\n",
      "train_loss: 0.21152        Accuracy 0.903\n",
      "train_loss: 0.21439        Accuracy 0.938\n",
      "train_loss: 0.17438        Accuracy 0.931\n",
      "train_loss: 0.30867        Accuracy 0.897\n",
      "train_loss: 0.22115        Accuracy 0.926\n",
      "train_loss: 0.31889        Accuracy 0.903\n",
      "train_loss: 0.25246        Accuracy 0.908\n",
      "train_loss: 0.28167        Accuracy 0.876\n",
      "train_loss: 0.20562        Accuracy 0.905\n",
      "train_loss: 0.19880        Accuracy 0.931\n",
      "train_loss: 0.28388        Accuracy 0.895\n",
      "train_loss: 0.36166        Accuracy 0.877\n",
      "train_loss: 0.23022        Accuracy 0.921\n",
      "train_loss: 0.25395        Accuracy 0.903\n",
      "train_loss: 0.24991        Accuracy 0.909\n",
      "train_loss: 0.28776        Accuracy 0.892\n",
      "train_loss: 0.28123        Accuracy 0.872\n",
      "train_loss: 0.33773        Accuracy 0.878\n",
      "train_loss: 0.22208        Accuracy 0.929\n",
      "train_loss: 0.35609        Accuracy 0.803\n",
      "train_loss: 0.24300        Accuracy 0.912\n",
      "train_loss: 0.31367        Accuracy 0.865\n",
      "train_loss: 0.21345        Accuracy 0.915\n",
      "train_loss: 0.37135        Accuracy 0.847\n",
      "train_loss: 0.26154        Accuracy 0.893\n",
      "train_loss: 0.28176        Accuracy 0.893\n",
      "train_loss: 0.27328        Accuracy 0.884\n",
      "train_loss: 0.26345        Accuracy 0.882\n",
      "train_loss: 0.21814        Accuracy 0.914\n",
      "train_loss: 0.27029        Accuracy 0.889\n",
      "train_loss: 0.17617        Accuracy 0.936\n",
      "train_loss: 0.28948        Accuracy 0.875\n",
      "train_loss: 0.29232        Accuracy 0.906\n",
      "train_loss: 0.27769        Accuracy 0.867\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa6692a020f4737a6d4469eb656c203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.077          MAP@50: 0.257          MAP_small: 0.000          MAP_large: -1.000\n"
     ]
    }
   ],
   "source": [
    "out_dict = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_map': [],\n",
    "    'test_map50': [],\n",
    "    'test_map_small': [],\n",
    "    'test_map_large': []\n",
    "}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    print(f\"EPOCH {epoch}/{NUM_EPOCHS}\")\n",
    "\n",
    "    train_loss = []\n",
    "    train_correct = 0\n",
    "    train_len = 0\n",
    "    \n",
    "    for minibatch_no, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        \n",
    "        images = [image for image, _, _ in batch]\n",
    "        bboxes = [bbox for _, bbox, _ in batch]\n",
    "        labels = [label for _, _, label in batch]\n",
    "        \n",
    "        # Selective search\n",
    "                \n",
    "        # Edge boxes\n",
    "        cropped_images_all, proposals_all, predictions_all = edge_boxes_train(images, bboxes, img_size = EDGE_BOXES_IMG_SIZE_OUT)         \n",
    "        if len(cropped_images_all) ==0:\n",
    "            print('no boxes detected')\n",
    "            continue \n",
    "        data, target = torch.stack(cropped_images_all).to(device), torch.FloatTensor(predictions_all).to(device)\n",
    "        \n",
    "        # CNN\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)[:,0]\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        predicted = output > 0.5\n",
    "        correct_in_the_item = (target==predicted).sum().cpu().item()\n",
    "        train_correct += correct_in_the_item\n",
    "        train_len += data.shape[0]\n",
    "        print(f'train_loss: {loss:.5f}        Accuracy {correct_in_the_item / len(target):.3f}')\n",
    "\n",
    "        # for now we break\n",
    "        \n",
    "    # Test evaluation\n",
    "    model.eval()\n",
    "    for batch in tqdm(val_loader, total=len(val_loader)): # Keep loader at 1 \n",
    "        test_images, test_bboxes, test_labels = batch[0]\n",
    "        \n",
    "        # Selective search\n",
    "\n",
    "        test_cropped_images_all, test_proposals_all = edge_boxes_test(test_images, test_bboxes, img_size = EDGE_BOXES_IMG_SIZE_OUT) \n",
    "        test_data = torch.stack(test_cropped_images_all).to(device)\n",
    "        test_proposals_all = test_proposals_all[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(test_data)[:,0]\n",
    "        \n",
    "        bboxes_xywh = torch.stack(test_proposals_all).to(device).to(float)\n",
    "        bboxes_xyxy = copy.deepcopy(bboxes_xywh)\n",
    "        bboxes_xyxy[:, 2] = bboxes_xyxy[:, 2] + bboxes_xyxy[:, 0]\n",
    "        bboxes_xyxy[:, 3] = bboxes_xyxy[:, 3] + bboxes_xyxy[:, 1]\n",
    "        \n",
    "        \n",
    "        bboxes_indices = torchvision.ops.nms(bboxes_xyxy, outputs, iou_threshold=0.1)\n",
    "        \n",
    "        final_bboxes = bboxes_xywh[bboxes_indices]\n",
    "        \n",
    "        outputs = outputs[bboxes_indices]\n",
    "        \n",
    "        # Reshaping\n",
    "        outputs = outputs.tolist()\n",
    "        \n",
    "        pred = [dict(\n",
    "            boxes=final_bboxes,\n",
    "            scores=torch.FloatTensor(outputs).to(device),\n",
    "            labels=torch.FloatTensor(np.ones(len(outputs))).to(device) # Simplification for Binary\n",
    "        )]\n",
    "        \n",
    "        target = [dict(\n",
    "            boxes=torch.FloatTensor(test_bboxes).to(device),\n",
    "            labels=torch.FloatTensor(test_labels).to(device)\n",
    "        ) ]\n",
    "                  \n",
    "        # Computing mAP\n",
    "        metric = MeanAveragePrecision(box_format='xywh')\n",
    "        metric.update(pred, target)\n",
    "    maps = metric.compute()\n",
    "    \n",
    "    out_dict['train_loss'].append(np.mean(train_loss))\n",
    "    out_dict['train_acc'].append(train_correct/train_len)\n",
    "    out_dict['test_map'].append(float(maps[\"map\"].detach().cpu()))\n",
    "    out_dict['test_map50'].append(float(maps[\"map_50\"].detach().cpu()))\n",
    "    out_dict['test_map_small'].append(float(maps[\"map_small\"].detach().cpu()))\n",
    "    out_dict['test_map_large'].append(float(maps[\"map_large\"].detach().cpu()))\n",
    "    \n",
    "    print(f'MAP: {float(maps[\"map\"].detach().cpu()):.3f}          MAP@50: {float(maps[\"map_50\"].detach().cpu()):.3f}          MAP_small: {float(maps[\"map_small\"].detach().cpu()):.3f}          MAP_large: {float(maps[\"map_large\"].detach().cpu()):.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2f49894",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'logs/resnet34_{NUM_EPOCHS}_epochs_img_size_{EDGE_BOXES_IMG_SIZE_IN}_in_{EDGE_BOXES_IMG_SIZE_OUT}_out', 'w') as csvFile:\n",
    "        writer = csv.writer(csvFile)\n",
    "        writer.writerow(out_dict.keys())\n",
    "        writer.writerows(zip(*out_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f21c5-9373-46e9-9b5e-ca3a8d2580a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(1)\n",
    "# img = test_images.detach().cpu()\n",
    "\n",
    "# img = denormalize(img)\n",
    "\n",
    "# plt.imshow(img.detach().cpu().permute(1,2,0))\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "\n",
    "# # Show annotations\n",
    "# for i, ann in enumerate(final_bboxes.cpu().numpy()):\n",
    "    \n",
    "#     [x, y, x1, y1] = ann\n",
    "#     if predictions_all[i]:\n",
    "#         edge_col = 'green'\n",
    "#     else:\n",
    "#         edge_col = 'red'\n",
    "#     rect = Rectangle((x,y),x1,y1,linewidth=2,edgecolor=edge_col,\n",
    "#                      facecolor='none', alpha=0.7)\n",
    "#     ax.add_patch(rect)\n",
    "# #plt.savefig(f'imgs/output_{img_idx}.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41f4f2-d34c-47c7-942d-cc1148cebd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = [image for image, _, _ in batch]\n",
    "# bboxes = [bbox for _, bbox, _ in batch]\n",
    "# labels = [label for _, _, label in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54188ffa-469d-472a-b552-b42190b12b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 3\n",
    "\n",
    "# cropped_images_all, proposals_all, predictions_all = edge_boxes_train([images[k]], [bboxes[k]])       \n",
    "# img = images[k]##.cpu().numpy()\n",
    "\n",
    "# #plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed67df-a4e6-4d68-b353-ca1c74479c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def denormalize(img):\n",
    "#     mean=[0.485, 0.456, 0.406]\n",
    "#     std=[0.229, 0.224, 0.225]\n",
    "    \n",
    "#     denormalize = transforms.Normalize(mean=[-0.485, -0.456, -0.406], \n",
    "#                          std=[1/0.229, 1/0.224, 1/0.225])\n",
    "\n",
    "#     denorm_image = denormalize(img)\n",
    "#     x = ((denorm_image - denorm_image.min())/(denorm_image - denorm_image.min()).max() * 255).to(torch.int64)\n",
    "#     return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
