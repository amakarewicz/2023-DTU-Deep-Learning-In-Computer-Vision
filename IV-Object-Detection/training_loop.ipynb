{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ffd46c-e3db-457c-b762-750d061a5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7d6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "# pip install torchsummary\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as fn\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "from matplotlib.patches import Polygon, Rectangle\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (6,6)\n",
    "\n",
    "# Own imports \n",
    "from config import * \n",
    "from utils import *\n",
    "from data_loader import TacoDataset\n",
    "from eval import *\n",
    "\n",
    "# speed-up using multithreads\n",
    "cv2.setUseOptimized(True);\n",
    "cv2.setNumThreads(8);\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c4e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 512\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(img_size, img_size),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=30, p=0.7),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.RGBShift(10, 10, 10, p=0.3),\n",
    "    A.GaussNoise(p=0.5),\n",
    "    A.Normalize(), # If you want to visualize - comment this line \n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='coco',\n",
    "                            label_fields=['labels'],\n",
    "                            min_visibility=0.3, # min visibility of the original area in case of a crop\n",
    "                           )\n",
    ")\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(img_size, img_size),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='coco',\n",
    "                            label_fields=['labels'],\n",
    "                           )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a040fe49-393f-44eb-bdfd-f0a5d9110564",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de156f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TacoDataset( 'train', transforms=train_transform, test_size=0.2) # test_transform for no augment\n",
    "valset   = TacoDataset('val', transforms=test_transform, test_size=0.2)\n",
    "testset  = TacoDataset('test', transforms=test_transform, test_size=0.2)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=12, collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)\n",
    "val_loader = DataLoader(valset, batch_size=1, shuffle=False, num_workers=12, collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)\n",
    "test_loader = DataLoader(testset, batch_size=1, shuffle=False, num_workers=12, collate_fn=lambda x: x)# persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8513dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "\n",
    "model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "model = transfer_model_set(model, freeze_convs=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), HEAD_LEARNING_RATE)\n",
    "\n",
    "lr_scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef75c06c-0cfb-4319-92f4-271666a7a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72397995-c9c9-40bb-83a8-22dea7a6cef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e36f12ca7c4e59a57d45aaaf6d43f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/8d/e/198218/dlincv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2113/1699717747.py:22: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  data, target = torch.stack(cropped_images_all).to(device), torch.FloatTensor(predictions_all).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.76457        Accuracy 0.474\n",
      "train_loss: 0.65831        Accuracy 0.611\n",
      "train_loss: 0.62202        Accuracy 0.676\n",
      "train_loss: 0.60418        Accuracy 0.659\n",
      "train_loss: 0.69045        Accuracy 0.736\n",
      "train_loss: 0.60795        Accuracy 0.703\n",
      "train_loss: 0.74124        Accuracy 0.746\n",
      "train_loss: 0.67892        Accuracy 0.724\n",
      "train_loss: 0.54729        Accuracy 0.768\n",
      "train_loss: 0.56384        Accuracy 0.758\n",
      "train_loss: 0.61574        Accuracy 0.706\n",
      "train_loss: 0.58896        Accuracy 0.759\n",
      "train_loss: 0.62376        Accuracy 0.705\n",
      "train_loss: 0.52742        Accuracy 0.795\n",
      "train_loss: 0.61352        Accuracy 0.804\n",
      "train_loss: 0.57046        Accuracy 0.820\n",
      "train_loss: 0.53997        Accuracy 0.793\n",
      "train_loss: 0.66717        Accuracy 0.753\n",
      "train_loss: 0.72338        Accuracy 0.792\n",
      "train_loss: 0.67434        Accuracy 0.659\n",
      "train_loss: 0.54291        Accuracy 0.722\n",
      "train_loss: 0.55612        Accuracy 0.753\n",
      "train_loss: 0.59109        Accuracy 0.721\n",
      "train_loss: 0.56328        Accuracy 0.734\n",
      "train_loss: 0.58120        Accuracy 0.743\n",
      "train_loss: 0.54847        Accuracy 0.736\n",
      "train_loss: 0.58075        Accuracy 0.684\n",
      "train_loss: 0.50857        Accuracy 0.822\n",
      "train_loss: 0.47977        Accuracy 0.888\n",
      "train_loss: 0.54030        Accuracy 0.843\n",
      "train_loss: 0.58795        Accuracy 0.825\n",
      "train_loss: 0.57339        Accuracy 0.812\n",
      "train_loss: 0.64514        Accuracy 0.746\n",
      "train_loss: 0.56752        Accuracy 0.823\n",
      "train_loss: 0.47287        Accuracy 0.868\n",
      "train_loss: 0.55882        Accuracy 0.802\n",
      "train_loss: 0.49136        Accuracy 0.860\n",
      "train_loss: 0.58005        Accuracy 0.824\n",
      "train_loss: 0.49251        Accuracy 0.840\n",
      "train_loss: 0.53378        Accuracy 0.764\n",
      "train_loss: 0.51003        Accuracy 0.829\n",
      "train_loss: 0.51751        Accuracy 0.806\n",
      "train_loss: 0.53967        Accuracy 0.803\n",
      "train_loss: 0.45729        Accuracy 0.899\n",
      "train_loss: 0.53681        Accuracy 0.827\n",
      "train_loss: 0.47234        Accuracy 0.841\n",
      "train_loss: 0.51335        Accuracy 0.811\n",
      "train_loss: 0.40358        Accuracy 0.890\n",
      "train_loss: 0.43204        Accuracy 0.844\n",
      "train_loss: 0.48843        Accuracy 0.859\n",
      "train_loss: 0.52867        Accuracy 0.737\n",
      "train_loss: 0.51126        Accuracy 0.804\n",
      "train_loss: 0.47588        Accuracy 0.827\n",
      "train_loss: 0.44875        Accuracy 0.850\n",
      "train_loss: 0.40843        Accuracy 0.866\n",
      "train_loss: 0.37770        Accuracy 0.901\n",
      "train_loss: 0.37589        Accuracy 0.909\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84192911887419591837b9b4ef51ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    print(f\"EPOCH {epoch}/{NUM_EPOCHS}\")\n",
    "\n",
    "    train_loss = []\n",
    "    train_correct = 0\n",
    "    train_len = 0\n",
    "    \n",
    "    for minibatch_no, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        \n",
    "        images = [image for image, _, _ in batch]\n",
    "        bboxes = [bbox for _, bbox, _ in batch]\n",
    "        labels = [label for _, _, label in batch]\n",
    "        \n",
    "        # Selective search\n",
    "                \n",
    "        # Edge boxes\n",
    "        cropped_images_all, proposals_all, predictions_all = edge_boxes_train(images, bboxes)         \n",
    "        if len(cropped_images_all) ==0:\n",
    "            print('no boxes detected')\n",
    "            continue \n",
    "        data, target = torch.stack(cropped_images_all).to(device), torch.FloatTensor(predictions_all).to(device)\n",
    "        \n",
    "        # CNN\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)[:,0]\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        predicted = output > 0.5\n",
    "        correct_in_the_item = (target==predicted).sum().cpu().item()\n",
    "        train_correct += correct_in_the_item\n",
    "        train_len += data.shape[0]\n",
    "        print(f'train_loss: {loss:.5f}        Accuracy {correct_in_the_item / len(target):.3f}')\n",
    "\n",
    "        # for now we break\n",
    "        \n",
    "    # Test evaluation\n",
    "    model.eval()\n",
    "    for batch in tqdm(val_loader, total=len(val_loader)): # Keep loader at 1 \n",
    "        test_images, test_bboxes, test_labels = batch[0]\n",
    "        \n",
    "        # Selective search\n",
    "\n",
    "        test_cropped_images_all, test_proposals_all = edge_boxes_test(test_images, test_bboxes) \n",
    "        test_data = torch.stack(test_cropped_images_all).to(device)\n",
    "        test_proposals_all = test_proposals_all[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(test_data)[:,0]\n",
    "        \n",
    "        bboxes_xywh = torch.stack(test_proposals_all).to(device).to(float)\n",
    "        bboxes_xyxy = copy.deepcopy(bboxes_xywh)\n",
    "        bboxes_xyxy[:, 2] = bboxes_xyxy[:, 2] + bboxes_xyxy[:, 0]\n",
    "        bboxes_xyxy[:, 3] = bboxes_xyxy[:, 3] + bboxes_xyxy[:, 1]\n",
    "        \n",
    "        \n",
    "        bboxes_indices = torchvision.ops.nms(bboxes_xyxy, outputs, iou_threshold=0.1)\n",
    "        \n",
    "        final_bboxes = bboxes_xywh[bboxes_indices]\n",
    "        \n",
    "        outputs = outputs[bboxes_indices]\n",
    "        \n",
    "        # Reshaping\n",
    "        outputs = outputs.tolist()\n",
    "        \n",
    "        pred = [dict(\n",
    "            boxes=final_bboxes,\n",
    "            scores=torch.FloatTensor(outputs).to(device),\n",
    "            labels=torch.FloatTensor(np.ones(len(outputs))).to(device) # Simplification for Binary\n",
    "        )]\n",
    "        \n",
    "        target = [dict(\n",
    "            boxes=torch.FloatTensor(test_bboxes).to(device),\n",
    "            labels=torch.FloatTensor(test_labels).to(device)\n",
    "        ) ]\n",
    "                  \n",
    "        # Computing mAP\n",
    "        metric = MeanAveragePrecision(box_format='xywh')\n",
    "        metric.update(pred, target)\n",
    "    maps = metric.compute()\n",
    "    print(f'MAP: {float(maps[\"map\"].detach().cpu()):.3f}          MAP@50: {float(maps[\"map_50\"].detach().cpu()):.3f}          MAP_small: {float(maps[\"map_small\"].detach().cpu()):.3f}          MAP_large: {float(maps[\"map_large\"].detach().cpu()):.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f21c5-9373-46e9-9b5e-ca3a8d2580a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(1)\n",
    "# img = test_images.detach().cpu()\n",
    "\n",
    "# img = denormalize(img)\n",
    "\n",
    "# plt.imshow(img.detach().cpu().permute(1,2,0))\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "\n",
    "# # Show annotations\n",
    "# for i, ann in enumerate(final_bboxes.cpu().numpy()):\n",
    "    \n",
    "#     [x, y, x1, y1] = ann\n",
    "#     if predictions_all[i]:\n",
    "#         edge_col = 'green'\n",
    "#     else:\n",
    "#         edge_col = 'red'\n",
    "#     rect = Rectangle((x,y),x1,y1,linewidth=2,edgecolor=edge_col,\n",
    "#                      facecolor='none', alpha=0.7)\n",
    "#     ax.add_patch(rect)\n",
    "# #plt.savefig(f'imgs/output_{img_idx}.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41f4f2-d34c-47c7-942d-cc1148cebd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = [image for image, _, _ in batch]\n",
    "# bboxes = [bbox for _, bbox, _ in batch]\n",
    "# labels = [label for _, _, label in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54188ffa-469d-472a-b552-b42190b12b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 3\n",
    "\n",
    "# cropped_images_all, proposals_all, predictions_all = edge_boxes_train([images[k]], [bboxes[k]])       \n",
    "# img = images[k]##.cpu().numpy()\n",
    "\n",
    "# #plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed67df-a4e6-4d68-b353-ca1c74479c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def denormalize(img):\n",
    "#     mean=[0.485, 0.456, 0.406]\n",
    "#     std=[0.229, 0.224, 0.225]\n",
    "    \n",
    "#     denormalize = transforms.Normalize(mean=[-0.485, -0.456, -0.406], \n",
    "#                          std=[1/0.229, 1/0.224, 1/0.225])\n",
    "\n",
    "#     denorm_image = denormalize(img)\n",
    "#     x = ((denorm_image - denorm_image.min())/(denorm_image - denorm_image.min()).max() * 255).to(torch.int64)\n",
    "#     return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
