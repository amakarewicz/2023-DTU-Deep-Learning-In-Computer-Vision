{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e881659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b87a5fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "transformations = transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                transforms.Resize((200,200), antialias=True)])\n",
    "\n",
    "trainset = datasets.ImageFolder('/dtu/datasets1/02514/hotdog_nothotdog/train', transform=transformations)\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "testset = datasets.ImageFolder('/dtu/datasets1/02514/hotdog_nothotdog/test', transform=transformations)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1acefb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "099e9a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.convolutional = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding='same'),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding='same'),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding='same'),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding='same'),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.BatchNorm2d(64)\n",
    "                )\n",
    "    \n",
    "        self.fully_connected = nn.Sequential(\n",
    "                nn.Linear(50*50*64, 500),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(500, 10))\n",
    "#                 nn.Softmax(dim=1))    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convolutional(x)\n",
    "        #reshape x so it becomes flat, except for the first dimension (which is the minibatch)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fully_connected(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b919f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, num_epochs=10):\n",
    "    \n",
    "    def loss_fun(output, target):\n",
    "        return F.cross_entropy(output, target)\n",
    "    \n",
    "    out_dict = {'train_acc': [],\n",
    "              'test_acc': [],\n",
    "              'train_loss': [],\n",
    "              'test_loss': []}\n",
    "  \n",
    "    for epoch in tqdm(range(num_epochs), unit='epoch'):\n",
    "        model.train()\n",
    "        #For each epoch\n",
    "        train_correct = 0\n",
    "        train_loss = []\n",
    "        for minibatch_no, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            #Zero the gradients computed for each weight\n",
    "            optimizer.zero_grad()\n",
    "            #Forward pass your image through the network\n",
    "            output = model(data)\n",
    "            #Compute the loss\n",
    "            loss = loss_fun(output, target)\n",
    "            #Backward pass through the network\n",
    "            loss.backward()\n",
    "            #Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            #Compute how many were correctly classified\n",
    "            predicted = output.argmax(1)\n",
    "            train_correct += (target==predicted).sum().cpu().item()\n",
    "            \n",
    "        #Comput the test accuracy\n",
    "        test_loss = []\n",
    "        test_correct = 0\n",
    "        model.eval()\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "            test_loss.append(loss_fun(output, target).cpu().item())\n",
    "            predicted = output.argmax(1)\n",
    "            test_correct += (target==predicted).sum().cpu().item()\n",
    "            \n",
    "        out_dict['train_acc'].append(train_correct/len(trainset))\n",
    "        out_dict['test_acc'].append(test_correct/len(testset))\n",
    "        out_dict['train_loss'].append(np.mean(train_loss))\n",
    "        out_dict['test_loss'].append(np.mean(test_loss))\n",
    "        \n",
    "        print(f\"Loss train: {np.mean(train_loss):.3f}\\t test: {np.mean(test_loss):.3f}\\t\",\n",
    "              f\"Accuracy train: {out_dict['train_acc'][-1]*100:.1f}%\\t test: {out_dict['test_acc'][-1]*100:.1f}%\")\n",
    "        \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd49933c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d02050cfee4116b7d8a8a23cf4b55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfbde7e0abd4e6dadc76639756ccf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 1.056\t test: 1.931\t Accuracy train: 60.3%\t test: 60.4%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715654c7414e4c0ca62e004004bdafc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.560\t test: 0.506\t Accuracy train: 74.2%\t test: 75.7%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f0d1b62e0344cd991a4444593c5e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.520\t test: 0.944\t Accuracy train: 74.0%\t test: 74.1%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9b6e481359492ba4e06570721312ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Network()\n",
    "model.to(device)\n",
    "out_dict_sgd = train(model, torch.optim.Adam(model.parameters(), lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae131e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
