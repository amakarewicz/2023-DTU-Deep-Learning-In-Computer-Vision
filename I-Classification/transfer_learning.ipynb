{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda43006-f2ab-40fb-9e22-af41693c9348",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa96e814-35ec-407e-b8c8-77c2bebbc8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from torchvision import datasets, models, transforms\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "from augmentation import *\n",
    "from train import train\n",
    "from config.transfer_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4079b174-fec5-4f57-9f55-7ef59a5b5a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6bffc2c-ca11-4dec-870a-e4a2e080fd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    ToTensor()\n",
       "    Resize(size=(300, 300), interpolation=bilinear, max_size=None, antialias=True)\n",
       "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60177308-b717-4c08-a8a4-88308a2864f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.ImageFolder('/dtu/datasets1/02514/hotdog_nothotdog/train', transform=transforms_0)\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "testset = datasets.ImageFolder('/dtu/datasets1/02514/hotdog_nothotdog/test', transform=transforms_0)\n",
    "test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb9cc3fb-0783-4e89-8c24-d18de90a547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a2180-d138-4671-ba19-ddc0b5ff24cd",
   "metadata": {},
   "source": [
    "# Options\n",
    "\n",
    "1. training only the head with normal learning rate\n",
    "2. training all with super low training rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "513be7bf-047e-4299-9680-b7264b001b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_model_set(model, freeze_convs=False,):\n",
    "    \n",
    "    if freeze_convs:\n",
    "        print('Freezing Convs')\n",
    "        # freeze the feature extractors\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    if type(model) == torchvision.models.densenet.DenseNet:\n",
    "        in_features = model.classifier.in_features\n",
    "    \n",
    "    elif type(model) == torchvision.models.resnet.ResNet:\n",
    "        in_features = model.fc.in_features\n",
    "    \n",
    "    \n",
    "    size_hidden = 512\n",
    "    out_features = 10\n",
    "    \n",
    "    head = nn.Sequential(\n",
    "                    nn.Linear(in_features, size_hidden),\n",
    "                    nn.Dropout(DROP_OUT_RATE),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm1d(size_hidden),\n",
    "                    nn.Linear(size_hidden, out_features))\n",
    "    \n",
    "    if type(model) == torchvision.models.densenet.DenseNet:\n",
    "        model.classifier = head\n",
    "    \n",
    "    elif type(model) == torchvision.models.resnet.ResNet:\n",
    "        model.fc = head\n",
    "\n",
    "    else:\n",
    "        raise Exception('Not implemented the classifier for this type of model')\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d514b93b-d527-49cc-8660-cb6c320d826a",
   "metadata": {},
   "source": [
    "# 1. Training just head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f75e414-52c7-47f1-8fc9-b28730b88950",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2) # lets take v2 here \n",
    "\n",
    "model = transfer_model_set(model, freeze_convs=True)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=HEAD_LEARNING_RATE)\n",
    "\n",
    "out_dict_resnet50 = train(model, train_loader, test_loader, loss, optimizer, NUM_EPOCHS)\n",
    "\n",
    "# saving results\n",
    "optim = 'Adam'\n",
    "d = out_dict_resnet50\n",
    "with open(f'results/results_transfer_resnet_head_{NUM_EPOCHS}_epochs_{HEAD_LEARNING_RATE:.0e}_lr_{optim}_optim.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerow(d.keys())\n",
    "    writer.writerows(zip(*d.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7dbe0734-e577-4be7-8d4e-578f32d5384a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing Convs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b786ead35b443eda19bb1fe0dae5b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436274584f874b5f82f4db7821d8ba3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 0.680\t test: 0.281\t Accuracy train: 86.2%\t test: 93.7%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061a2b626b254d108c19cf25b57b8da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/appl/python/3.10.11/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/appl/python/3.10.11/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/zhome/8d/e/198218/dlincv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 51, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/zhome/8d/e/198218/dlincv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/appl/python/3.10.11/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/zhome/8d/e/198218/dlincv/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 307, in rebuild_storage_fd\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mHEAD_LEARNING_RATE)\n\u001b[0;32m----> 8\u001b[0m out_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# saving results\u001b[39;00m\n\u001b[1;32m     11\u001b[0m optim \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/2023-DTU-Deep-Learning-In-Computer-Vision/I-Classification/train.py:31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, loss_function, optimizer, num_epochs, lr_scheduler)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 31\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     32\u001b[0m predicted \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m train_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (target\u001b[38;5;241m==\u001b[39mpredicted)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    fd = df.detach()\n",
      "  File \"/appl/python/3.10.11/lib/python3.10/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/appl/python/3.10.11/lib/python3.10/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/appl/python/3.10.11/lib/python3.10/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/appl/python/3.10.11/lib/python3.10/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "model = models.densenet121(weights=torchvision.models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "\n",
    "model = transfer_model_set(model, freeze_convs=True)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=HEAD_LEARNING_RATE)\n",
    "\n",
    "out_dict = train(model, train_loader, test_loader, loss, optimizer, NUM_EPOCHS)\n",
    "\n",
    "# saving results\n",
    "optim = 'Adam'\n",
    "d = out_dict\n",
    "with open(f'results/results_transfer_densenet_head_{NUM_EPOCHS}_epochs_{HEAD_LEARNING_RATE:.0e}_lr_{optim}_optim.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerow(d.keys())\n",
    "    writer.writerows(zip(*d.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c9ada-6823-4cc8-b0a8-94fcf72effbf",
   "metadata": {},
   "source": [
    "# Train everything (but slowly! 🐢🐢🐢)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d26ff95e-fda9-40d5-9876-62ac18be8d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /zhome/8d/e/198218/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|████████████████████████████████████████████████| 97.8M/97.8M [00:01<00:00, 85.9MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec0ea57ab9e41c992b222b0844eae27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Pin memory thread exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mHEAD_LEARNING_RATE)\n\u001b[0;32m----> 8\u001b[0m out_dict_resnet50 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# saving results\u001b[39;00m\n\u001b[1;32m     11\u001b[0m optim \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/2023-DTU-Deep-Learning-In-Computer-Vision/I-Classification/train.py:22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, loss_function, optimizer, num_epochs, lr_scheduler)\u001b[0m\n\u001b[1;32m     20\u001b[0m train_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     21\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m minibatch_no, (data, target) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)):\n\u001b[1;32m     23\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/dlincv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:438\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/dlincv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1111\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m   1109\u001b[0m resume_iteration_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m resume_iteration_cnt \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1111\u001b[0m     return_idx, return_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_idx, _utils\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39m_ResumeIteration):\n\u001b[1;32m   1113\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m return_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dlincv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1289\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1286\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1288\u001b[0m         \u001b[38;5;66;03m# while condition is false, i.e., pin_memory_thread died.\u001b[39;00m\n\u001b[0;32m-> 1289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPin memory thread exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Pin memory thread exited unexpectedly"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2) # lets take v2 here \n",
    "\n",
    "model = transfer_model_set(model, freeze_convs=False)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=HEAD_LEARNING_RATE)\n",
    "\n",
    "out_dict_resnet50 = train(model, train_loader, test_loader, loss, optimizer, NUM_EPOCHS)\n",
    "\n",
    "# saving results\n",
    "optim = 'Adam'\n",
    "d = out_dict_resnet50\n",
    "with open(f'results/results_transfer_resnet_full_{NUM_EPOCHS}_epochs_{HEAD_LEARNING_RATE:.0e}_lr_{optim}_optim.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerow(d.keys())\n",
    "    writer.writerows(zip(*d.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0838bf-6f26-4cff-8733-0c2a9ab5bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.densenet121(weights=torchvision.models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "\n",
    "model = transfer_model_set(model, freeze_convs=False)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=HEAD_LEARNING_RATE)\n",
    "\n",
    "out_dict = train(model, train_loader, test_loader, loss, optimizer, NUM_EPOCHS)\n",
    "\n",
    "# saving results\n",
    "optim = 'Adam'\n",
    "d = out_dict\n",
    "with open(f'results/results_transfer_densenet_full_{NUM_EPOCHS}_epochs_{HEAD_LEARNING_RATE:.0e}_lr_{optim}_optim.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerow(d.keys())\n",
    "    writer.writerows(zip(*d.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
